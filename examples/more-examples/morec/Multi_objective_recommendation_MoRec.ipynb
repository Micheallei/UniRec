{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>Copyright (c) Microsoft Corporation.</i>\n",
    "\n",
    "<i>Licensed under the MIT license.</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MoRec: A Data-Centric Multi-Objective Learning Framework for Responsible Recommendation Systems\n",
    "\n",
    "MoRec[[1]](https://arxiv.org/abs/2310.13260v1) is a data-centric multi-objective framework designed for responsible recommendation systems. Concretely, MoRec adopts a tri-level framework to optimize diverse objectives simultaneously, comprising a PID-based objective coordinator for trade-off among objectives and an adaptive data sampler for unified objective modeling. \n",
    "\n",
    "\n",
    "## Strengths of MoRec\n",
    "- MoRec is model-agnostic, which is capable of converting an accuracy-oriented model to multi-objective model\n",
    "- MoRec adopts a post-training strategy, which is able to convert a well-trained model to multi-objective model at a low cost\n",
    "- MoRec exhibit great capability in objective controlling, which could optimize model with objective preference without sacrificing too much accuracy\n",
    "\n",
    "## Data requirements\n",
    "\n",
    "MoRec is capable of optimizing accuracy, revenue, fairness and alignment objectives simultaneously. \n",
    "\n",
    "- For accuracy, basical user-item interaction files are required, including `train.csv`, `valid.csv`, `test.csv` and `user_history.csv`. \n",
    "  `train.csv`, `valid.csv`, `test.csv` represent interactions in training set, validation set and test set respectively, which are formatted as follows:\n",
    "\n",
    "  <table>\n",
    "      <tr>\n",
    "          <td>user_id</td>\n",
    "          <td>item_id</td>\n",
    "      </tr>\n",
    "      <tr>\n",
    "          <td>1</td>\n",
    "          <td>1</td>\n",
    "      </tr>\n",
    "      <tr>\n",
    "          <td>1</td>\n",
    "          <td>2</td>\n",
    "      </tr>\n",
    "      <tr>\n",
    "          <td>...</td>\n",
    "          <td>...</td>\n",
    "      </tr>\n",
    "      <tr>\n",
    "          <td>100</td>\n",
    "          <td>254</td>\n",
    "      </tr>\n",
    "      <tr>\n",
    "          <td>...</td>\n",
    "          <td>...</td>\n",
    "      </tr>\n",
    "  </table>\n",
    "\n",
    "  `user_history.csv` represents the user's interaction history, consist of interactions in training set and validation set, which is formatted as follows:\n",
    "\n",
    "  <table>\n",
    "      <tr>\n",
    "          <td>user_id</td>\n",
    "          <td>item_seq</td>\n",
    "      </tr>\n",
    "      <tr>\n",
    "          <td>1</td>\n",
    "          <td>1,2,3,...</td>\n",
    "      </tr>\n",
    "      <tr>\n",
    "          <td>...</td>\n",
    "          <td>...</td>\n",
    "      </tr>\n",
    "      <tr>\n",
    "          <td>100</td>\n",
    "          <td>254,257,327,...</td>\n",
    "      </tr>\n",
    "      <tr>\n",
    "          <td>...</td>\n",
    "          <td>...</td>\n",
    "      </tr>\n",
    "  </table>\n",
    "\n",
    "- For revenue, MoRec would sample data samples according to their weights, i.e. item price. For fairness, MoRec aims to improve the accuracy preformance of the most disadvantaged group. For alignment, MoRec targets on aligning the model's distribution with some pre-defined expectation distribution. To model those objectives, `item_meta_morec_filename` is required to provide item weights, fairness group and alignment group. And if you want to set the pre-set distribution for alignment,  `align_dist_filename` is needed. By default, the expected distribution to aligned with is the distribution derived from the training set. Here are the example of item_meta_morec_file  and align_dist_file.\n",
    "  \n",
    "  - item_meta_morec_file: `item_meta_morec.csv`, columns separated by comma\n",
    "\n",
    "  <table>\n",
    "      <tr>\n",
    "          <td>item_id</td>\n",
    "          <td>weight</td>\n",
    "          <td>fair_group</td>\n",
    "          <td>align_group</td>\n",
    "      </tr>\n",
    "      <tr>\n",
    "          <td>1</td>\n",
    "          <td>2.35</td>\n",
    "          <td>1</td>\n",
    "          <td>2</td>\n",
    "      </tr>\n",
    "      <tr>\n",
    "          <td>2</td>\n",
    "          <td>63.21</td>\n",
    "          <td>5</td>\n",
    "          <td>1</td>\n",
    "      </tr>\n",
    "      <tr>\n",
    "          <td>...</td>\n",
    "          <td>...</td>\n",
    "          <td>...</td>\n",
    "          <td>...</td>\n",
    "      </tr>\n",
    "      <tr>\n",
    "          <td>100</td>\n",
    "          <td>5.89</td>\n",
    "          <td>5</td>\n",
    "          <td>4</td>\n",
    "      </tr>\n",
    "      <tr>\n",
    "          <td>...</td>\n",
    "          <td>...</td>\n",
    "          <td>...</td>\n",
    "          <td>...</td>\n",
    "      </tr>\n",
    "  </table>\n",
    "\n",
    "  - align_dist_file: `expected_align_dist.csv`, columns separated by comma\n",
    "\n",
    "  <table>\n",
    "      <tr>\n",
    "          <td>group_id</td>\n",
    "          <td>proportion</td>\n",
    "      </tr>\n",
    "      <tr>\n",
    "          <td>1</td>\n",
    "          <td>0.21</td>\n",
    "      </tr>\n",
    "      <tr>\n",
    "          <td>2</td>\n",
    "          <td>0.12</td>\n",
    "      </tr>\n",
    "      <tr>\n",
    "          <td>3</td>\n",
    "          <td>0.33</td>\n",
    "      </tr>\n",
    "      <tr>\n",
    "          <td>4</td>\n",
    "          <td>0.22</td>\n",
    "      </tr>\n",
    "      <tr>\n",
    "          <td>5</td>\n",
    "          <td>0.12</td>\n",
    "      </tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Amazon Electronics dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Preparation\n",
    "\n",
    "We put the script for downloading and preprocessing ml-100k into the our [example folder](../../preprocess/download_split_ml100k.py). Here we would call the functions defined in the script. The preprocessed csv files would be saved in `~/.unirec/dataset/ml-100k`. \n",
    "\n",
    "We believe that you could easily process your own dataset to obtain `train.csv`, `valid.csv`, `test.csv` and `user_history.csv` using leave-one-out strategy. \n",
    "\n",
    "As for the columns in `item_meta_morec.csv` file, we set price of items as weight and group items according to their categories as the fair_group. As for align_group, we divide items according to the popularity, where items with similar popularity are put into the same group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/v-huangxu/.local/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import shutil\n",
    "import datetime\n",
    "from copy import deepcopy\n",
    "\n",
    "sys.path.append(os.path.abspath(\"../../\"))\n",
    "from preprocess.download_split_amazon import preprocess_amazon\n",
    "from preprocess.prepare_data import process_transaction_dataset\n",
    "\n",
    "import unirec\n",
    "from unirec.main import main\n",
    "from unirec.constants.protocols import DataFileFormat\n",
    "\n",
    "UNIREC_PATH = os.path.dirname(unirec.__file__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/anaconda/envs/new-unirec/lib/python3.9/site-packages/unirec'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "UNIREC_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original dataset size: (7824482, 9)\n",
      "filter by rating>3 dataset size: (5833322, 9)\n",
      "drop_duplicates dataset size: (5833322, 9)\n",
      "Ite: 0, users: 177149 / 3256144, items: 51997 / 410110\n",
      "Ite: 1, users: 130234 / 177149, items: 45697 / 51997\n",
      "Ite: 2, users: 125581 / 130234, items: 44973 / 45697\n",
      "Ite: 3, users: 125011 / 125581, items: 44866 / 44973\n",
      "Ite: 4, users: 124916 / 125011, items: 44847 / 44866\n",
      "k-core filtered dataset size: (1072840, 9)\n",
      "#Users: 124916, #Items: 44847\n",
      "size in Train/Valid/Test: (823008, 2) / (124916, 2) / (124916, 2)\n",
      "0 raws price not float\n",
      "44847/44847 item prices existing in meta df\n",
      "Map dict saved in /home/v-huangxu/.unirec/dataset/Electronics/map.json.\n",
      "Item meta file saved in /home/v-huangxu/.unirec/dataset/Electronics/item_meta_morec.csv.\n"
     ]
    }
   ],
   "source": [
    "preprocess_amazon(\"Electronics\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Binary Data File Preparation\n",
    "\n",
    "Upon the interaction files are processed, UniRec requires to convert them into binary files for time-saving loading. We provide the tools in [example folder](../../preprocess/prepare_data.py) to easily obtain the pickle file.\n",
    "\n",
    "Note that the function `process_transaction_dataset` requires some meta information of the csv files, such as the directory path, the seperator, header , file format and so on. \n",
    "\n",
    "Note that we have defined several data formats in UniRec, you can list all formats using codes below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFileFormat.T1: user-item\n",
      "DataFileFormat.T2: user-item-label\n",
      "DataFileFormat.T2_1: user-item-label-session\n",
      "DataFileFormat.T3: user-item-rating\n",
      "DataFileFormat.T4: user-item_group-label_group\n",
      "DataFileFormat.T5: user-item_seq\n",
      "DataFileFormat.T5_1: user_item_seq\n",
      "DataFileFormat.T6: user-item_seq-time_seq\n",
      "DataFileFormat.T7: label-index_group-value_group\n"
     ]
    }
   ],
   "source": [
    "# All supported data file formats\n",
    "for format in DataFileFormat.__members__.values():\n",
    "    print(f\"{format}: {format.value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_data_folder_path = os.path.expanduser(\"~/.unirec/dataset/binary/\")\n",
    "\n",
    "UNIREC_PATH = os.path.dirname(unirec.__file__)\n",
    "\n",
    "BINARY_FILE_CONFIG = {\n",
    "    \"raw_datapath\": os.path.expanduser(\"~/.unirec/dataset/Electronics\"), # the dir of csv files\n",
    "    \"outpathroot\": binary_data_folder_path, # the output dir of processed binary files\n",
    "    \"dataset_name\": \"Electronics\", # the dataset name, set as you like\n",
    "    \"example_yaml_file\": os.path.join(UNIREC_PATH, \"config/dataset/example.yaml\"), # Do not modify the value\n",
    "    \"index_by_zero\": 0,  # whether the user_id and item_id start from 0\n",
    "    \"sep\": \"\\t\" ,   # the seperator of csv files \n",
    "    \"train_file\": 'train.csv',  # the filename of training csv file\n",
    "    \"train_file_format\": 'user-item', \n",
    "    \"train_file_has_header\": 1, # whether the training file has header\n",
    "    \"train_file_col_names\": \"['user_id', 'item_id']\",  # the columns of training csv file\n",
    "    \"train_neg_k\": 0,  \n",
    "    \"valid_file\": 'valid.csv', # the filename of validation csv file\n",
    "    \"valid_file_format\": 'user-item', \n",
    "    \"valid_file_has_header\": 1, # whether the validation file has header\n",
    "    \"valid_file_col_names\": \"['user_id', 'item_id']\", # the columns of validation csv file\n",
    "    \"valid_neg_k\": 0, \n",
    "    \"test_file\": 'test.csv', # the filename of test csv file\n",
    "    \"test_file_format\": 'user-item', \n",
    "    \"test_file_has_header\": 1, # whether the test file has header\n",
    "    \"test_file_col_names\": \"['user_id', 'item_id']\", # the columns of test csv file\n",
    "    \"test_neg_k\": 0, \n",
    "    \"user_history_file\": 'user_history.csv', # the filename of history csv file\n",
    "    \"user_history_file_format\": 'user-item_seq', \n",
    "    \"user_history_file_has_header\": 1, # whether the history file has header\n",
    "    \"user_history_file_col_names\": \"['user_id', 'item_seq']\" # the columns of history csv file\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data shape of train.csv is (823008, 2)\n",
      "data dtypes is user_id    int64\n",
      "item_id    int64\n",
      "dtype: object\n",
      "saving train.pkl at 28/10/2023 11:35:20\n",
      "finish saving train.pkl at 28/10/2023 11:35:20\n",
      "In saving:\n",
      "   user_id  item_id\n",
      "0        1        1\n",
      "1        1        2\n",
      "2        1        3\n",
      "3        1        4\n",
      "4        2        7\n",
      "data.shape=(823008, 2)\n",
      "\n",
      "data shape of valid.csv is (124916, 2)\n",
      "data dtypes is user_id    int64\n",
      "item_id    int64\n",
      "dtype: object\n",
      "saving valid.pkl at 28/10/2023 11:35:20\n",
      "finish saving valid.pkl at 28/10/2023 11:35:20\n",
      "In saving:\n",
      "   user_id  item_id\n",
      "0        1        5\n",
      "1        2       10\n",
      "2        3       17\n",
      "3        4       23\n",
      "4        5       29\n",
      "data.shape=(124916, 2)\n",
      "\n",
      "data shape of test.csv is (124916, 2)\n",
      "data dtypes is user_id    int64\n",
      "item_id    int64\n",
      "dtype: object\n",
      "saving test.pkl at 28/10/2023 11:35:20\n",
      "finish saving test.pkl at 28/10/2023 11:35:20\n",
      "In saving:\n",
      "   user_id  item_id\n",
      "0        1        6\n",
      "1        2       11\n",
      "2        3       18\n",
      "3        4       24\n",
      "4        5       30\n",
      "data.shape=(124916, 2)\n",
      "\n",
      "data shape of user_history.csv is (124916, 2)\n",
      "data dtypes is user_id      int64\n",
      "item_seq    object\n",
      "dtype: object\n",
      "saving user_history.pkl at 28/10/2023 11:35:21\n",
      "finish saving user_history.pkl at 28/10/2023 11:35:22\n",
      "In saving:\n",
      "   user_id                  item_seq\n",
      "0        1           [1, 2, 3, 4, 5]\n",
      "1        2             [7, 8, 9, 10]\n",
      "2        3  [12, 13, 14, 15, 16, 17]\n",
      "3        4      [19, 20, 21, 22, 23]\n",
      "4        5      [25, 26, 27, 28, 29]\n",
      "data.shape=(124916, 2)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "process_transaction_dataset(BINARY_FILE_CONFIG)    # the binary files would be saved in `binary_data_folder_path`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/v-huangxu/.unirec/dataset/binary/Electronics/item_meta_morec.csv'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for the item_meta_morec.csv file, we copy it to the binary file path as well\n",
    "shutil.copyfile(os.path.join(BINARY_FILE_CONFIG['raw_datapath'], 'item_meta_morec.csv'), os.path.join(BINARY_FILE_CONFIG['outpathroot'], BINARY_FILE_CONFIG['dataset_name'], 'item_meta_morec.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MoRec pretraining stage: accuracy-oriented model training\n",
    "\n",
    "Since MoRec provides a post-training strategy to convert a single-objective model (usually an accuracy-oriented model) to a multi-objective model, we need to train the accuracy-oriented model first.\n",
    "\n",
    "1. First, setup morec_configurations, including hyperparameters, file paths.\n",
    "2. Second, training with unirec's user-friendly interface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"Electronics\"\n",
    "model = \"MF\"\n",
    "ckpt_output_path = os.path.expanduser(\"~/.unirec/output\")\n",
    "\n",
    "GLOBAL_CONF = {\n",
    "    'config_dir': f\"{os.path.join(UNIREC_PATH, 'config')}\",\n",
    "    'exp_name': '',\n",
    "    'checkpoint_dir': datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\"),\n",
    "    'model': model,\n",
    "    'dataloader': 'BaseDataset',\n",
    "    'dataset': dataset,\n",
    "    'dataset_path': os.path.join(BINARY_FILE_CONFIG['outpathroot'], dataset),\n",
    "    'output_path': os.path.join(ckpt_output_path, dataset, model),\n",
    "    'learning_rate': 0.001,\n",
    "    'scheduler': None,\n",
    "    'dropout_prob': 0.0,\n",
    "    'embedding_size': 64,\n",
    "    'user_pre_item_emb': 0,\n",
    "    'loss_type': 'bpr',\n",
    "    'max_seq_len': 20,\n",
    "    'has_user_bias': 0,\n",
    "    'has_item_bias': 0,\n",
    "    'epochs': 200,\n",
    "    'early_stop': 10,\n",
    "    'batch_size': 512,\n",
    "    'valid_batch_size': 1024,\n",
    "    'n_sample_neg_train': 10,\n",
    "    'neg_by_pop_alpha': 1.0,\n",
    "    'valid_protocol': 'one_vs_all',\n",
    "    'test_protocol': 'one_vs_all',\n",
    "    'grad_clip_value': -1,\n",
    "    'weight_decay': 1e-6,\n",
    "    'history_mask_mode': 'autoagressive',\n",
    "    'user_history_filename': \"user_history\",\n",
    "    'metrics': \"['hit@10', 'ndcg@10', 'rhit@10', 'rndcg@10', 'pop-kl@10', 'least-misery']\",\n",
    "    'key_metric': \"ndcg@10\",\n",
    "    'num_workers': 4,\n",
    "    'num_workers_test': 0,\n",
    "    'verbose': 2,\n",
    "    'item_meta_morec_filename': 'item_meta_morec.csv',\n",
    "    'align_dist_filename': None,  # the expected alignment distribution is set as the distribution derived from the training set\n",
    "    'use_tensorboard': 1,\n",
    "    'seed': 2022  # random seed\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load configuration files from /anaconda/envs/new-unirec/lib/python3.9/site-packages/unirec/config\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] MF-MoRec-Pretrain: config={'gpu_id': 0, 'use_gpu': True, 'seed': 2022, 'state': 'INFO', 'verbose': 2, 'saved': True, 'use_tensorboard': 1, 'use_wandb': False, 'init_method': 'normal', 'init_std': 0.02, 'init_mean': 0.0, 'scheduler': None, 'scheduler_factor': 0.1, 'time_seq': 0, 'seq_last': False, 'has_user_emb': True, 'has_user_bias': 0, 'has_item_bias': 0, 'use_features': False, 'use_text_emb': False, 'use_position_emb': True, 'load_pretrained_model': False, 'embedding_size': 64, 'hidden_size': 128, 'inner_size': 128, 'dropout_prob': 0.0, 'epochs': 200, 'batch_size': 512, 'learning_rate': 0.001, 'optimizer': 'adam', 'eval_step': 1, 'early_stop': 10, 'clip_grad_norm': None, 'weight_decay': 1e-06, 'num_workers': 4, 'persistent_workers': False, 'pin_memory': False, 'shuffle_train': False, 'use_pre_item_emb': 0, 'loss_type': 'bpr', 'ccl_w': 150, 'ccl_m': 0.4, 'distance_type': 'dot', 'metrics': \"['hit@10', 'ndcg@10', 'rhit@10', 'rndcg@10', 'pop-kl@10', 'least-misery']\", 'key_metric': 'ndcg@10', 'test_protocol': 'one_vs_all', 'valid_protocol': 'one_vs_all', 'test_batch_size': 100, 'model': 'MF', 'dataloader': 'BaseDataset', 'max_seq_len': 20, 'history_mask_mode': 'autoagressive', 'tau': 1.0, 'enable_morec': 0, 'morec_objectives': ['fairness', 'alignment', 'revenue'], 'morec_objective_controller': 'PID', 'morec_ngroup': [10, 10, -1], 'morec_alpha': 0.1, 'morec_lambda': 0.2, 'morec_expect_loss': 0.2, 'morec_beta_min': 0.6, 'morec_beta_max': 1.3, 'morec_K_p': 0.01, 'morec_K_i': 0.001, 'morec_objective_weights': '[0.3,0.3,0.4]', 'group_size': -1, 'n_items': 44848, 'n_neg_test_from_sampling': 0, 'n_neg_train_from_sampling': 0, 'n_neg_valid_from_sampling': 0, 'n_users': 124917, 'test_file_format': 'user-item', 'train_file_format': 'user-item', 'user_history_file_format': 'user-item_seq', 'valid_file_format': 'user-item', 'config_dir': '/anaconda/envs/new-unirec/lib/python3.9/site-packages/unirec/config', 'exp_name': 'MF-MoRec-Pretrain', 'checkpoint_dir': 'morec_pretrain_2023-10-28_11-35-22', 'dataset': 'Electronics', 'dataset_path': '/home/v-huangxu/.unirec/dataset/binary/Electronics', 'output_path': '/home/v-huangxu/.unirec/output/Electronics/MF', 'user_pre_item_emb': 0, 'valid_batch_size': 1024, 'n_sample_neg_train': 10, 'neg_by_pop_alpha': 1.0, 'grad_clip_value': -1, 'user_history_filename': 'user_history', 'num_workers_test': 0, 'item_meta_morec_filename': 'item_meta_morec.csv', 'align_dist_filename': None, 'cmd_args': {'config_dir': '/anaconda/envs/new-unirec/lib/python3.9/site-packages/unirec/config', 'exp_name': 'MF-MoRec-Pretrain', 'checkpoint_dir': 'morec_pretrain_2023-10-28_11-35-22', 'model': 'MF', 'dataloader': 'BaseDataset', 'dataset': 'Electronics', 'dataset_path': '/home/v-huangxu/.unirec/dataset/binary/Electronics', 'output_path': '/home/v-huangxu/.unirec/output/Electronics/MF', 'learning_rate': 0.001, 'scheduler': None, 'dropout_prob': 0.0, 'embedding_size': 64, 'user_pre_item_emb': 0, 'loss_type': 'bpr', 'max_seq_len': 20, 'has_user_bias': 0, 'has_item_bias': 0, 'epochs': 200, 'early_stop': 10, 'batch_size': 512, 'valid_batch_size': 1024, 'n_sample_neg_train': 10, 'neg_by_pop_alpha': 1.0, 'valid_protocol': 'one_vs_all', 'test_protocol': 'one_vs_all', 'grad_clip_value': -1, 'weight_decay': 1e-06, 'history_mask_mode': 'autoagressive', 'user_history_filename': 'user_history', 'metrics': \"['hit@10', 'ndcg@10', 'rhit@10', 'rndcg@10', 'pop-kl@10', 'least-misery']\", 'key_metric': 'ndcg@10', 'num_workers': 4, 'num_workers_test': 0, 'verbose': 2, 'item_meta_morec_filename': 'item_meta_morec.csv', 'align_dist_filename': None, 'use_tensorboard': 1, 'seed': 2022, 'logger_time_str': '2023-10-28_113522', 'logger_rand': 7}, 'device': device(type='cuda'), 'task': 'train', 'logger_time_str': '2023-10-28_113522', 'logger_rand': 7}\n",
      "[INFO] MF-MoRec-Pretrain: Loading user history from user_history ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing logs to /home/v-huangxu/.unirec/output/Electronics/MF/MF-MoRec-Pretrain.2023-10-28_113522.7.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] MF-MoRec-Pretrain: Done. 124917 of users have history.\n",
      "[INFO] MF-MoRec-Pretrain: Constructing dataset of task type: train\n",
      "[DEBUG] MF-MoRec-Pretrain: loading train at 28/10/2023 11:35:25\n",
      "[DEBUG] MF-MoRec-Pretrain: Finished loading train at 28/10/2023 11:35:25\n",
      "[INFO] MF-MoRec-Pretrain: Finished initializing <class 'unirec.data.dataset.basedataset.BaseDataset'>\n",
      "[INFO] MF-MoRec-Pretrain: Constructing dataset of task type: valid\n",
      "[DEBUG] MF-MoRec-Pretrain: loading valid at 28/10/2023 11:35:25\n",
      "[DEBUG] MF-MoRec-Pretrain: Finished loading valid at 28/10/2023 11:35:25\n",
      "[INFO] MF-MoRec-Pretrain: Finished initializing <class 'unirec.data.dataset.basedataset.BaseDataset'>\n",
      "[INFO] MF-MoRec-Pretrain: MF(\n",
      "  (scorer_layers): InnerProductScorer()\n",
      "  (user_embedding): Embedding(124917, 64, padding_idx=0)\n",
      "  (item_embedding): Embedding(44848, 64, padding_idx=0)\n",
      ")\n",
      "Trainable parameter number: 10864960\n",
      "All trainable parameters:\n",
      "user_embedding.weight : torch.Size([124917, 64])\n",
      "item_embedding.weight : torch.Size([44848, 64])\n",
      "[INFO] MF-MoRec-Pretrain: tensorboard log file saved in /home/v-huangxu/.unirec/output/Electronics/MF/morec_pretrain_2023-10-28_11-35-22\n",
      "[DEBUG] MF-MoRec-Pretrain: >> Valid before training...\n",
      "[INFO] MF-MoRec-Pretrain: one_vs_all\n",
      "Evaluate: 100%|██████████| 122/122 [01:27<00:00,  1.39it/s]\n",
      "[INFO] MF-MoRec-Pretrain: epoch 0 evaluating [time: 87.62s, ndcg@10: 0.000111]\n",
      "[INFO] MF-MoRec-Pretrain: complete scores on valid set: \n",
      "hit@10:0.00024816676806814177 min-hit@10:0.0 min-ndcg@10:0.0 min-rhit@10:0.0 min-rndcg@10:0.0 ndcg@10:0.00011126678854110222 pop-kl@10:0.6680214467375609 rhit@10:0.018153799353165325 rndcg@10:0.007915046841297556\n",
      "[INFO] MF-MoRec-Pretrain: Saving best model at epoch 0 to /home/v-huangxu/.unirec/output/Electronics/MF/morec_pretrain_2023-10-28_11-35-22/MF-MoRec-Pretrain.pth\n",
      "[INFO] MF-MoRec-Pretrain: \n",
      ">> epoch 1\n",
      "Train: 100%|██████████| 1608/1608 [00:11<00:00, 137.15it/s]\n",
      "[INFO] MF-MoRec-Pretrain: epoch 1 training [time: 11.73s, train loss: 1114.5808]\n",
      "[INFO] MF-MoRec-Pretrain: one_vs_all\n",
      "Evaluate: 100%|██████████| 122/122 [01:17<00:00,  1.58it/s]\n",
      "[INFO] MF-MoRec-Pretrain: epoch 1 evaluating [time: 77.16s, ndcg@10: 0.000794]\n",
      "[INFO] MF-MoRec-Pretrain: complete scores on valid set: \n",
      "hit@10:0.0017371673764769926 min-hit@10:0.0 min-ndcg@10:0.0 min-rhit@10:0.0 min-rndcg@10:0.0 ndcg@10:0.0007944490961756569 pop-kl@10:0.3834596448731298 rhit@10:0.06344943802235102 rndcg@10:0.027152205343501074\n",
      "[INFO] MF-MoRec-Pretrain: Saving best model at epoch 1 to /home/v-huangxu/.unirec/output/Electronics/MF/morec_pretrain_2023-10-28_11-35-22/MF-MoRec-Pretrain.pth\n",
      "[INFO] MF-MoRec-Pretrain: \n",
      ">> epoch 2\n",
      "Train: 100%|██████████| 1608/1608 [00:10<00:00, 149.40it/s]\n",
      "[INFO] MF-MoRec-Pretrain: epoch 2 training [time: 10.77s, train loss: 1112.6129]\n",
      "[INFO] MF-MoRec-Pretrain: one_vs_all\n",
      "Evaluate: 100%|██████████| 122/122 [01:18<00:00,  1.55it/s]\n",
      "[INFO] MF-MoRec-Pretrain: epoch 2 evaluating [time: 78.71s, ndcg@10: 0.009870]\n",
      "[INFO] MF-MoRec-Pretrain: complete scores on valid set: \n",
      "hit@10:0.018020109513593136 min-hit@10:0.0 min-ndcg@10:0.0 min-rhit@10:0.0 min-rndcg@10:0.0 ndcg@10:0.009869698592821198 pop-kl@10:5.674750620659745 rhit@10:0.7510887316276539 rndcg@10:0.43170314614499455\n",
      "[INFO] MF-MoRec-Pretrain: Saving best model at epoch 2 to /home/v-huangxu/.unirec/output/Electronics/MF/morec_pretrain_2023-10-28_11-35-22/MF-MoRec-Pretrain.pth\n",
      "[INFO] MF-MoRec-Pretrain: \n",
      ">> epoch 3\n",
      "Train: 100%|██████████| 1608/1608 [00:10<00:00, 148.88it/s]\n",
      "[INFO] MF-MoRec-Pretrain: epoch 3 training [time: 10.80s, train loss: 1055.7458]\n",
      "[INFO] MF-MoRec-Pretrain: one_vs_all\n",
      "Evaluate: 100%|██████████| 122/122 [01:22<00:00,  1.49it/s]\n",
      "[INFO] MF-MoRec-Pretrain: epoch 3 evaluating [time: 82.04s, ndcg@10: 0.011468]\n",
      "[INFO] MF-MoRec-Pretrain: complete scores on valid set: \n",
      "hit@10:0.020685900925421885 min-hit@10:0.0 min-ndcg@10:0.0 min-rhit@10:0.0 min-rndcg@10:0.0 ndcg@10:0.011468167609617741 pop-kl@10:6.482447908800175 rhit@10:1.1634314259182168 rndcg@10:0.6333786585163262\n",
      "[INFO] MF-MoRec-Pretrain: Saving best model at epoch 3 to /home/v-huangxu/.unirec/output/Electronics/MF/morec_pretrain_2023-10-28_11-35-22/MF-MoRec-Pretrain.pth\n",
      "[INFO] MF-MoRec-Pretrain: \n",
      ">> epoch 4\n",
      "Train: 100%|██████████| 1608/1608 [00:10<00:00, 148.64it/s]\n",
      "[INFO] MF-MoRec-Pretrain: epoch 4 training [time: 10.82s, train loss: 947.3856]\n",
      "[INFO] MF-MoRec-Pretrain: one_vs_all\n",
      "Evaluate: 100%|██████████| 122/122 [01:21<00:00,  1.50it/s]\n",
      "[INFO] MF-MoRec-Pretrain: epoch 4 evaluating [time: 81.42s, ndcg@10: 0.011791]\n",
      "[INFO] MF-MoRec-Pretrain: complete scores on valid set: \n",
      "hit@10:0.020870024656569213 min-hit@10:0.0 min-ndcg@10:0.0 min-rhit@10:0.0 min-rndcg@10:0.0 ndcg@10:0.011791233950792445 pop-kl@10:6.5338076833429986 rhit@10:1.510981219379423 rndcg@10:0.808808207151228\n",
      "[INFO] MF-MoRec-Pretrain: Saving best model at epoch 4 to /home/v-huangxu/.unirec/output/Electronics/MF/morec_pretrain_2023-10-28_11-35-22/MF-MoRec-Pretrain.pth\n",
      "[INFO] MF-MoRec-Pretrain: \n",
      ">> epoch 5\n",
      "Train: 100%|██████████| 1608/1608 [00:10<00:00, 148.14it/s]\n",
      "[INFO] MF-MoRec-Pretrain: epoch 5 training [time: 10.86s, train loss: 838.9582]\n",
      "[INFO] MF-MoRec-Pretrain: one_vs_all\n",
      "Evaluate: 100%|██████████| 122/122 [01:21<00:00,  1.50it/s]\n",
      "[INFO] MF-MoRec-Pretrain: epoch 5 evaluating [time: 81.09s, ndcg@10: 0.012190]\n",
      "[INFO] MF-MoRec-Pretrain: complete scores on valid set: \n",
      "hit@10:0.021510455025777322 min-hit@10:0.0003627130939426913 min-ndcg@10:0.00022884648297840317 min-rhit@10:0.0060210373594486765 min-rndcg@10:0.003798851617441493 ndcg@10:0.012189512209136248 pop-kl@10:6.589015419234851 rhit@10:1.7743720580229911 rndcg@10:0.9413826179492792\n",
      "[INFO] MF-MoRec-Pretrain: Saving best model at epoch 5 to /home/v-huangxu/.unirec/output/Electronics/MF/morec_pretrain_2023-10-28_11-35-22/MF-MoRec-Pretrain.pth\n",
      "[INFO] MF-MoRec-Pretrain: \n",
      ">> epoch 6\n",
      "Train: 100%|██████████| 1608/1608 [00:10<00:00, 148.76it/s]\n",
      "[INFO] MF-MoRec-Pretrain: epoch 6 training [time: 10.81s, train loss: 742.3507]\n",
      "[INFO] MF-MoRec-Pretrain: one_vs_all\n",
      "Evaluate: 100%|██████████| 122/122 [01:20<00:00,  1.51it/s]\n",
      "[INFO] MF-MoRec-Pretrain: epoch 6 evaluating [time: 80.63s, ndcg@10: 0.012356]\n",
      "[INFO] MF-MoRec-Pretrain: complete scores on valid set: \n",
      "hit@10:0.021550481923852827 min-hit@10:0.001088139281828074 min-ndcg@10:0.0003188827161043732 min-rhit@10:0.03353282553500181 min-rndcg@10:0.00993597911425559 ndcg@10:0.012356435180487844 pop-kl@10:6.638023581021681 rhit@10:1.8563408210317331 rndcg@10:1.0158411150568383\n",
      "[INFO] MF-MoRec-Pretrain: Saving best model at epoch 6 to /home/v-huangxu/.unirec/output/Electronics/MF/morec_pretrain_2023-10-28_11-35-22/MF-MoRec-Pretrain.pth\n",
      "[INFO] MF-MoRec-Pretrain: \n",
      ">> epoch 7\n",
      "Train: 100%|██████████| 1608/1608 [00:13<00:00, 120.34it/s]\n",
      "[INFO] MF-MoRec-Pretrain: epoch 7 training [time: 13.36s, train loss: 657.8518]\n",
      "[INFO] MF-MoRec-Pretrain: one_vs_all\n",
      "Evaluate: 100%|██████████| 122/122 [01:22<00:00,  1.48it/s]\n",
      "[INFO] MF-MoRec-Pretrain: epoch 7 evaluating [time: 82.43s, ndcg@10: 0.012021]\n",
      "[INFO] MF-MoRec-Pretrain: complete scores on valid set: \n",
      "hit@10:0.020990105350795735 min-hit@10:0.0029017047515415306 min-ndcg@10:0.0011057144075753169 min-rhit@10:0.09145447950671019 min-rndcg@10:0.034703888239023754 ndcg@10:0.012020896921502752 pop-kl@10:5.778331765927806 rhit@10:1.8490716961798328 rndcg@10:1.026083923779224\n",
      "[INFO] MF-MoRec-Pretrain: No better score in the epoch. Patience: 1 / 10\n",
      "[INFO] MF-MoRec-Pretrain: \n",
      ">> epoch 8\n",
      "Train: 100%|██████████| 1608/1608 [00:10<00:00, 146.99it/s]\n",
      "[INFO] MF-MoRec-Pretrain: epoch 8 training [time: 10.94s, train loss: 586.1568]\n",
      "[INFO] MF-MoRec-Pretrain: one_vs_all\n",
      "Evaluate: 100%|██████████| 122/122 [01:22<00:00,  1.48it/s]\n",
      "[INFO] MF-MoRec-Pretrain: epoch 8 evaluating [time: 82.31s, ndcg@10: 0.011960]\n",
      "[INFO] MF-MoRec-Pretrain: complete scores on valid set: \n",
      "hit@10:0.02084600851772391 min-hit@10:0.003989844033369605 min-ndcg@10:0.002031061620933069 min-rhit@10:0.11984766050054407 min-rndcg@10:0.062044949115705475 ndcg@10:0.011959666019878204 pop-kl@10:5.557146877464247 rhit@10:1.8389420890838646 rndcg@10:1.0028107347383688\n",
      "[INFO] MF-MoRec-Pretrain: No better score in the epoch. Patience: 2 / 10\n",
      "[INFO] MF-MoRec-Pretrain: \n",
      ">> epoch 9\n",
      "Train: 100%|██████████| 1608/1608 [00:11<00:00, 140.14it/s]\n",
      "[INFO] MF-MoRec-Pretrain: epoch 9 training [time: 11.48s, train loss: 526.8066]\n",
      "[INFO] MF-MoRec-Pretrain: one_vs_all\n",
      "Evaluate: 100%|██████████| 122/122 [01:21<00:00,  1.51it/s]\n",
      "[INFO] MF-MoRec-Pretrain: epoch 9 evaluating [time: 81.06s, ndcg@10: 0.011930]\n",
      "[INFO] MF-MoRec-Pretrain: complete scores on valid set: \n",
      "hit@10:0.0207979762400333 min-hit@10:0.005077983315197679 min-ndcg@10:0.003007282040397827 min-rhit@10:0.13803409503083064 min-rndcg@10:0.08417889110327055 ndcg@10:0.011929737925092715 pop-kl@10:4.321490547902395 rhit@10:1.8090617695091102 rndcg@10:1.0131115441306235\n",
      "[INFO] MF-MoRec-Pretrain: No better score in the epoch. Patience: 3 / 10\n",
      "[INFO] MF-MoRec-Pretrain: \n",
      ">> epoch 10\n",
      "Train: 100%|██████████| 1608/1608 [00:13<00:00, 122.41it/s]\n",
      "[INFO] MF-MoRec-Pretrain: epoch 10 training [time: 13.14s, train loss: 478.9679]\n",
      "[INFO] MF-MoRec-Pretrain: one_vs_all\n",
      "Evaluate: 100%|██████████| 122/122 [01:20<00:00,  1.51it/s]\n",
      "[INFO] MF-MoRec-Pretrain: epoch 10 evaluating [time: 80.92s, ndcg@10: 0.011871]\n",
      "[INFO] MF-MoRec-Pretrain: complete scores on valid set: \n",
      "hit@10:0.020693906305036983 min-hit@10:0.005803409503083061 min-ndcg@10:0.0036642014041481912 min-rhit@10:0.1549220166848023 min-rndcg@10:0.10021667351659759 ndcg@10:0.011871428572661887 pop-kl@10:3.9632320752642816 rhit@10:1.8002283934804186 rndcg@10:1.0173730924730913\n",
      "[INFO] MF-MoRec-Pretrain: No better score in the epoch. Patience: 4 / 10\n",
      "[INFO] MF-MoRec-Pretrain: \n",
      ">> epoch 11\n",
      "Train: 100%|██████████| 1608/1608 [00:10<00:00, 147.31it/s]\n",
      "[INFO] MF-MoRec-Pretrain: epoch 11 training [time: 10.92s, train loss: 441.1356]\n",
      "[INFO] MF-MoRec-Pretrain: one_vs_all\n",
      "Evaluate: 100%|██████████| 122/122 [01:20<00:00,  1.52it/s]\n",
      "[INFO] MF-MoRec-Pretrain: epoch 11 evaluating [time: 80.31s, ndcg@10: 0.012101]\n",
      "[INFO] MF-MoRec-Pretrain: complete scores on valid set: \n",
      "hit@10:0.02114220756348266 min-hit@10:0.007254261878853826 min-ndcg@10:0.004102904401022993 min-rhit@10:0.18690605730866883 min-rndcg@10:0.11861748516930064 ndcg@10:0.012100876953919234 pop-kl@10:3.0420956711894624 rhit@10:1.822050417880816 rndcg@10:1.0146509407911057\n",
      "[INFO] MF-MoRec-Pretrain: No better score in the epoch. Patience: 5 / 10\n",
      "[INFO] MF-MoRec-Pretrain: \n",
      ">> epoch 12\n",
      "Train: 100%|██████████| 1608/1608 [00:11<00:00, 135.16it/s]\n",
      "[INFO] MF-MoRec-Pretrain: epoch 12 training [time: 11.90s, train loss: 411.9941]\n",
      "[INFO] MF-MoRec-Pretrain: one_vs_all\n",
      "Evaluate: 100%|██████████| 122/122 [01:21<00:00,  1.49it/s]\n",
      "[INFO] MF-MoRec-Pretrain: epoch 12 evaluating [time: 81.85s, ndcg@10: 0.012307]\n",
      "[INFO] MF-MoRec-Pretrain: complete scores on valid set: \n",
      "hit@10:0.021422395850011208 min-hit@10:0.007254261878853826 min-ndcg@10:0.00464744990311888 min-rhit@10:0.19379760609357996 min-rndcg@10:0.13482758427306935 ndcg@10:0.012306986019161654 pop-kl@10:2.5416674563866026 rhit@10:1.9022463895737933 rndcg@10:1.067623938104885\n",
      "[INFO] MF-MoRec-Pretrain: No better score in the epoch. Patience: 6 / 10\n",
      "[INFO] MF-MoRec-Pretrain: \n",
      ">> epoch 13\n",
      "Train: 100%|██████████| 1608/1608 [00:11<00:00, 140.75it/s]\n",
      "[INFO] MF-MoRec-Pretrain: epoch 13 training [time: 11.43s, train loss: 388.7665]\n",
      "[INFO] MF-MoRec-Pretrain: one_vs_all\n",
      "Evaluate: 100%|██████████| 122/122 [01:20<00:00,  1.52it/s]\n",
      "[INFO] MF-MoRec-Pretrain: epoch 13 evaluating [time: 80.39s, ndcg@10: 0.012661]\n",
      "[INFO] MF-MoRec-Pretrain: complete scores on valid set: \n",
      "hit@10:0.022182906913445835 min-hit@10:0.00797968806673921 min-ndcg@10:0.00428873882234067 min-rhit@10:0.2245665578527385 min-rndcg@10:0.12386550019141464 ndcg@10:0.012661442270036046 pop-kl@10:2.7276133429651774 rhit@10:1.9634880239520958 rndcg@10:1.099918060492915\n",
      "[INFO] MF-MoRec-Pretrain: Saving best model at epoch 13 to /home/v-huangxu/.unirec/output/Electronics/MF/morec_pretrain_2023-10-28_11-35-22/MF-MoRec-Pretrain.pth\n",
      "[INFO] MF-MoRec-Pretrain: \n",
      ">> epoch 14\n",
      "Train: 100%|██████████| 1608/1608 [00:11<00:00, 141.43it/s]\n",
      "[INFO] MF-MoRec-Pretrain: epoch 14 training [time: 11.37s, train loss: 370.7000]\n",
      "[INFO] MF-MoRec-Pretrain: one_vs_all\n",
      "Evaluate: 100%|██████████| 122/122 [01:19<00:00,  1.53it/s]\n",
      "[INFO] MF-MoRec-Pretrain: epoch 14 evaluating [time: 79.60s, ndcg@10: 0.012690]\n",
      "[INFO] MF-MoRec-Pretrain: complete scores on valid set: \n",
      "hit@10:0.022294982228057255 min-hit@10:0.007616974972796518 min-ndcg@10:0.004695898638496866 min-rhit@10:0.21007616974972798 min-rndcg@10:0.12964823372762868 ndcg@10:0.012690410457564003 pop-kl@10:2.145120543486839 rhit@10:1.9579450190528036 rndcg@10:1.0874534469497863\n",
      "[INFO] MF-MoRec-Pretrain: Saving best model at epoch 14 to /home/v-huangxu/.unirec/output/Electronics/MF/morec_pretrain_2023-10-28_11-35-22/MF-MoRec-Pretrain.pth\n",
      "[INFO] MF-MoRec-Pretrain: \n",
      ">> epoch 15\n",
      "Train: 100%|██████████| 1608/1608 [00:12<00:00, 131.21it/s]\n",
      "[INFO] MF-MoRec-Pretrain: epoch 15 training [time: 12.26s, train loss: 356.3612]\n",
      "[INFO] MF-MoRec-Pretrain: one_vs_all\n",
      "Evaluate: 100%|██████████| 122/122 [01:19<00:00,  1.53it/s]\n",
      "[INFO] MF-MoRec-Pretrain: epoch 15 evaluating [time: 79.71s, ndcg@10: 0.012869]\n",
      "[INFO] MF-MoRec-Pretrain: complete scores on valid set: \n",
      "hit@10:0.022367030644593167 min-hit@10:0.008705114254624592 min-ndcg@10:0.0050404959950538465 min-rhit@10:0.2848277112803772 min-rndcg@10:0.1473639677470027 ndcg@10:0.012869385344919514 pop-kl@10:2.349626076747196 rhit@10:1.992711502129431 rndcg@10:1.1034087548875329\n",
      "[INFO] MF-MoRec-Pretrain: Saving best model at epoch 15 to /home/v-huangxu/.unirec/output/Electronics/MF/morec_pretrain_2023-10-28_11-35-22/MF-MoRec-Pretrain.pth\n",
      "[INFO] MF-MoRec-Pretrain: \n",
      ">> epoch 16\n",
      "Train: 100%|██████████| 1608/1608 [00:14<00:00, 109.16it/s]\n",
      "[INFO] MF-MoRec-Pretrain: epoch 16 training [time: 14.73s, train loss: 345.1427]\n",
      "[INFO] MF-MoRec-Pretrain: one_vs_all\n",
      "Evaluate: 100%|██████████| 122/122 [01:19<00:00,  1.54it/s]\n",
      "[INFO] MF-MoRec-Pretrain: epoch 16 evaluating [time: 79.19s, ndcg@10: 0.013027]\n",
      "[INFO] MF-MoRec-Pretrain: complete scores on valid set: \n",
      "hit@10:0.02269525120881232 min-hit@10:0.007616974972796518 min-ndcg@10:0.00524800066456789 min-rhit@10:0.24236488937250636 min-rndcg@10:0.15897066629959875 ndcg@10:0.013027185424680824 pop-kl@10:1.9377723129703486 rhit@10:2.010131208171891 rndcg@10:1.1361186096142721\n",
      "[INFO] MF-MoRec-Pretrain: Saving best model at epoch 16 to /home/v-huangxu/.unirec/output/Electronics/MF/morec_pretrain_2023-10-28_11-35-22/MF-MoRec-Pretrain.pth\n",
      "[INFO] MF-MoRec-Pretrain: \n",
      ">> epoch 17\n",
      "Train: 100%|██████████| 1608/1608 [00:10<00:00, 148.12it/s]\n",
      "[INFO] MF-MoRec-Pretrain: epoch 17 training [time: 10.86s, train loss: 335.9074]\n",
      "[INFO] MF-MoRec-Pretrain: one_vs_all\n",
      "Evaluate: 100%|██████████| 122/122 [01:19<00:00,  1.54it/s]\n",
      "[INFO] MF-MoRec-Pretrain: epoch 17 evaluating [time: 79.17s, ndcg@10: 0.013083]\n",
      "[INFO] MF-MoRec-Pretrain: complete scores on valid set: \n",
      "hit@10:0.02269525120881232 min-hit@10:0.008705114254624592 min-ndcg@10:0.00509875265830106 min-rhit@10:0.27622415669205663 min-rndcg@10:0.15542787282819023 ndcg@10:0.013083192831770452 pop-kl@10:2.165968354864943 rhit@10:1.986767587819014 rndcg@10:1.1207508917355984\n",
      "[INFO] MF-MoRec-Pretrain: Saving best model at epoch 17 to /home/v-huangxu/.unirec/output/Electronics/MF/morec_pretrain_2023-10-28_11-35-22/MF-MoRec-Pretrain.pth\n",
      "[INFO] MF-MoRec-Pretrain: \n",
      ">> epoch 18\n",
      "Train: 100%|██████████| 1608/1608 [00:10<00:00, 147.80it/s]\n",
      "[INFO] MF-MoRec-Pretrain: epoch 18 training [time: 10.88s, train loss: 328.8635]\n",
      "[INFO] MF-MoRec-Pretrain: one_vs_all\n",
      "Evaluate: 100%|██████████| 122/122 [01:21<00:00,  1.50it/s]\n",
      "[INFO] MF-MoRec-Pretrain: epoch 18 evaluating [time: 81.57s, ndcg@10: 0.013354]\n",
      "[INFO] MF-MoRec-Pretrain: complete scores on valid set: \n",
      "hit@10:0.0231515578468731 min-hit@10:0.009067827348567283 min-ndcg@10:0.005000131309112305 min-rhit@10:0.2787123685165035 min-rndcg@10:0.13767636237957562 ndcg@10:0.013354139620700221 pop-kl@10:1.809598793265304 rhit@10:2.0491623170770756 rndcg@10:1.1521321992341198\n",
      "[INFO] MF-MoRec-Pretrain: Saving best model at epoch 18 to /home/v-huangxu/.unirec/output/Electronics/MF/morec_pretrain_2023-10-28_11-35-22/MF-MoRec-Pretrain.pth\n",
      "[INFO] MF-MoRec-Pretrain: \n",
      ">> epoch 19\n",
      "Train: 100%|██████████| 1608/1608 [00:11<00:00, 141.77it/s]\n",
      "[INFO] MF-MoRec-Pretrain: epoch 19 training [time: 11.34s, train loss: 322.7937]\n",
      "[INFO] MF-MoRec-Pretrain: one_vs_all\n",
      "Evaluate: 100%|██████████| 122/122 [01:21<00:00,  1.49it/s]\n",
      "[INFO] MF-MoRec-Pretrain: epoch 19 evaluating [time: 81.93s, ndcg@10: 0.013362]\n",
      "[INFO] MF-MoRec-Pretrain: complete scores on valid set: \n",
      "hit@10:0.02346376765186205 min-hit@10:0.011244105912223431 min-ndcg@10:0.005680887479180238 min-rhit@10:0.3877548059484947 min-rndcg@10:0.1731999564972501 ndcg@10:0.013361779348927516 pop-kl@10:1.910164556444878 rhit@10:2.0835519068814246 rndcg@10:1.157789054035867\n",
      "[INFO] MF-MoRec-Pretrain: Saving best model at epoch 19 to /home/v-huangxu/.unirec/output/Electronics/MF/morec_pretrain_2023-10-28_11-35-22/MF-MoRec-Pretrain.pth\n",
      "[INFO] MF-MoRec-Pretrain: \n",
      ">> epoch 20\n",
      "Train: 100%|██████████| 1608/1608 [00:10<00:00, 148.66it/s]\n",
      "[INFO] MF-MoRec-Pretrain: epoch 20 training [time: 10.82s, train loss: 317.8282]\n",
      "[INFO] MF-MoRec-Pretrain: one_vs_all\n",
      "Evaluate: 100%|██████████| 122/122 [01:21<00:00,  1.50it/s]\n",
      "[INFO] MF-MoRec-Pretrain: epoch 20 evaluating [time: 81.23s, ndcg@10: 0.013309]\n",
      "[INFO] MF-MoRec-Pretrain: complete scores on valid set: \n",
      "hit@10:0.023191584744948604 min-hit@10:0.01088139281828074 min-ndcg@10:0.0057258217581137106 min-rhit@10:0.3121508886470802 min-rndcg@10:0.1517060836980671 ndcg@10:0.013308588735230959 pop-kl@10:1.7423489180648408 rhit@10:2.072052179064331 rndcg@10:1.1535929842385533\n",
      "[INFO] MF-MoRec-Pretrain: No better score in the epoch. Patience: 1 / 10\n",
      "[INFO] MF-MoRec-Pretrain: \n",
      ">> epoch 21\n",
      "Train: 100%|██████████| 1608/1608 [00:10<00:00, 148.06it/s]\n",
      "[INFO] MF-MoRec-Pretrain: epoch 21 training [time: 10.86s, train loss: 313.9417]\n",
      "[INFO] MF-MoRec-Pretrain: one_vs_all\n",
      "Evaluate: 100%|██████████| 122/122 [01:21<00:00,  1.49it/s]\n",
      "[INFO] MF-MoRec-Pretrain: epoch 21 evaluating [time: 82.00s, ndcg@10: 0.013630]\n",
      "[INFO] MF-MoRec-Pretrain: complete scores on valid set: \n",
      "hit@10:0.02399212270645874 min-hit@10:0.011606819006166122 min-ndcg@10:0.0059086225269592094 min-rhit@10:0.3634312658686979 min-rndcg@10:0.17133771077850354 ndcg@10:0.013629785767081732 pop-kl@10:1.6703932754591246 rhit@10:2.1782687566044383 rndcg@10:1.194309803699611\n",
      "[INFO] MF-MoRec-Pretrain: Saving best model at epoch 21 to /home/v-huangxu/.unirec/output/Electronics/MF/morec_pretrain_2023-10-28_11-35-22/MF-MoRec-Pretrain.pth\n",
      "[INFO] MF-MoRec-Pretrain: \n",
      ">> epoch 22\n",
      "Train: 100%|██████████| 1608/1608 [00:10<00:00, 148.94it/s]\n",
      "[INFO] MF-MoRec-Pretrain: epoch 22 training [time: 10.80s, train loss: 310.4167]\n",
      "[INFO] MF-MoRec-Pretrain: one_vs_all\n",
      "Evaluate: 100%|██████████| 122/122 [01:21<00:00,  1.50it/s]\n",
      "[INFO] MF-MoRec-Pretrain: epoch 22 evaluating [time: 81.27s, ndcg@10: 0.013776]\n",
      "[INFO] MF-MoRec-Pretrain: complete scores on valid set: \n",
      "hit@10:0.02428832175221749 min-hit@10:0.012332245194051506 min-ndcg@10:0.0060808088991735655 min-rhit@10:0.39368153790351834 min-rndcg@10:0.18000606962937865 ndcg@10:0.013776211100197966 pop-kl@10:1.6117566986395258 rhit@10:2.164372218130584 rndcg@10:1.1883997330087923\n",
      "[INFO] MF-MoRec-Pretrain: Saving best model at epoch 22 to /home/v-huangxu/.unirec/output/Electronics/MF/morec_pretrain_2023-10-28_11-35-22/MF-MoRec-Pretrain.pth\n",
      "[INFO] MF-MoRec-Pretrain: \n",
      ">> epoch 23\n",
      "Train: 100%|██████████| 1608/1608 [00:10<00:00, 148.49it/s]\n",
      "[INFO] MF-MoRec-Pretrain: epoch 23 training [time: 10.83s, train loss: 307.5884]\n",
      "[INFO] MF-MoRec-Pretrain: one_vs_all\n",
      "Evaluate: 100%|██████████| 122/122 [01:20<00:00,  1.52it/s]\n",
      "[INFO] MF-MoRec-Pretrain: epoch 23 evaluating [time: 80.24s, ndcg@10: 0.013926]\n",
      "[INFO] MF-MoRec-Pretrain: complete scores on valid set: \n",
      "hit@10:0.02421627333568158 min-hit@10:0.011244105912223431 min-ndcg@10:0.006065673033188583 min-rhit@10:0.33002176278563655 min-rndcg@10:0.1736868375747981 ndcg@10:0.013926187614309707 pop-kl@10:1.606911912518525 rhit@10:2.1672518332319317 rndcg@10:1.2116728323855317\n",
      "[INFO] MF-MoRec-Pretrain: Saving best model at epoch 23 to /home/v-huangxu/.unirec/output/Electronics/MF/morec_pretrain_2023-10-28_11-35-22/MF-MoRec-Pretrain.pth\n",
      "[INFO] MF-MoRec-Pretrain: \n",
      ">> epoch 24\n",
      "Train: 100%|██████████| 1608/1608 [00:10<00:00, 147.04it/s]\n",
      "[INFO] MF-MoRec-Pretrain: epoch 24 training [time: 10.94s, train loss: 305.0636]\n",
      "[INFO] MF-MoRec-Pretrain: one_vs_all\n",
      "Evaluate: 100%|██████████| 122/122 [01:20<00:00,  1.52it/s]\n",
      "[INFO] MF-MoRec-Pretrain: epoch 24 evaluating [time: 80.03s, ndcg@10: 0.013887]\n",
      "[INFO] MF-MoRec-Pretrain: complete scores on valid set: \n",
      "hit@10:0.02430433251144769 min-hit@10:0.012332245194051506 min-ndcg@10:0.006409074415957311 min-rhit@10:0.35619876677548057 min-rndcg@10:0.1826891156261677 ndcg@10:0.013887235263033409 pop-kl@10:1.5433649289282096 rhit@10:2.198496509654488 rndcg@10:1.22323165018625\n",
      "[INFO] MF-MoRec-Pretrain: No better score in the epoch. Patience: 1 / 10\n",
      "[INFO] MF-MoRec-Pretrain: \n",
      ">> epoch 25\n",
      "Train: 100%|██████████| 1608/1608 [00:10<00:00, 148.70it/s]\n",
      "[INFO] MF-MoRec-Pretrain: epoch 25 training [time: 10.82s, train loss: 302.7984]\n",
      "[INFO] MF-MoRec-Pretrain: one_vs_all\n",
      "Evaluate: 100%|██████████| 122/122 [01:20<00:00,  1.52it/s]\n",
      "[INFO] MF-MoRec-Pretrain: epoch 25 evaluating [time: 80.48s, ndcg@10: 0.014067]\n",
      "[INFO] MF-MoRec-Pretrain: complete scores on valid set: \n",
      "hit@10:0.024576515418361138 min-hit@10:0.012332245194051506 min-ndcg@10:0.006654886906301904 min-rhit@10:0.3753971708378673 min-rndcg@10:0.1851689610592869 ndcg@10:0.014066927777846221 pop-kl@10:1.5442751541509137 rhit@10:2.2088413814083063 rndcg@10:1.2486398157141458\n",
      "[INFO] MF-MoRec-Pretrain: Saving best model at epoch 25 to /home/v-huangxu/.unirec/output/Electronics/MF/morec_pretrain_2023-10-28_11-35-22/MF-MoRec-Pretrain.pth\n",
      "[INFO] MF-MoRec-Pretrain: \n",
      ">> epoch 26\n",
      "Train: 100%|██████████| 1608/1608 [00:10<00:00, 148.46it/s]\n",
      "[INFO] MF-MoRec-Pretrain: epoch 26 training [time: 10.83s, train loss: 301.0760]\n",
      "[INFO] MF-MoRec-Pretrain: one_vs_all\n",
      "Evaluate: 100%|██████████| 122/122 [01:20<00:00,  1.52it/s]\n",
      "[INFO] MF-MoRec-Pretrain: epoch 26 evaluating [time: 80.49s, ndcg@10: 0.014177]\n",
      "[INFO] MF-MoRec-Pretrain: complete scores on valid set: \n",
      "hit@10:0.02482468218642928 min-hit@10:0.011606819006166122 min-ndcg@10:0.007052644297034165 min-rhit@10:0.32981138919114983 min-rndcg@10:0.186470465003595 ndcg@10:0.014177318707093359 pop-kl@10:1.5108189650514352 rhit@10:2.2131995901245634 rndcg@10:1.2484838032531183\n",
      "[INFO] MF-MoRec-Pretrain: Saving best model at epoch 26 to /home/v-huangxu/.unirec/output/Electronics/MF/morec_pretrain_2023-10-28_11-35-22/MF-MoRec-Pretrain.pth\n",
      "[INFO] MF-MoRec-Pretrain: \n",
      ">> epoch 27\n",
      "Train: 100%|██████████| 1608/1608 [00:10<00:00, 148.99it/s]\n",
      "[INFO] MF-MoRec-Pretrain: epoch 27 training [time: 10.80s, train loss: 299.5471]\n",
      "[INFO] MF-MoRec-Pretrain: one_vs_all\n",
      "Evaluate: 100%|██████████| 122/122 [01:20<00:00,  1.52it/s]\n",
      "[INFO] MF-MoRec-Pretrain: epoch 27 evaluating [time: 80.44s, ndcg@10: 0.014064]\n",
      "[INFO] MF-MoRec-Pretrain: complete scores on valid set: \n",
      "hit@10:0.02459252617759134 min-hit@10:0.012332245194051506 min-ndcg@10:0.006081249149701971 min-rhit@10:0.40776931447225245 min-rndcg@10:0.1806318773986344 ndcg@10:0.014063544373569962 pop-kl@10:1.5356545650155056 rhit@10:2.2073865637708536 rndcg@10:1.2245172910447433\n",
      "[INFO] MF-MoRec-Pretrain: No better score in the epoch. Patience: 1 / 10\n",
      "[INFO] MF-MoRec-Pretrain: \n",
      ">> epoch 28\n",
      "Train: 100%|██████████| 1608/1608 [00:10<00:00, 148.42it/s]\n",
      "[INFO] MF-MoRec-Pretrain: epoch 28 training [time: 10.84s, train loss: 298.0129]\n",
      "[INFO] MF-MoRec-Pretrain: one_vs_all\n",
      "Evaluate: 100%|██████████| 122/122 [01:20<00:00,  1.52it/s]\n",
      "[INFO] MF-MoRec-Pretrain: epoch 28 evaluating [time: 80.21s, ndcg@10: 0.014170]\n",
      "[INFO] MF-MoRec-Pretrain: complete scores on valid set: \n",
      "hit@10:0.02458452079797624 min-hit@10:0.011606819006166122 min-ndcg@10:0.005933125359592242 min-rhit@10:0.3928908233587233 min-rndcg@10:0.17623893526497983 ndcg@10:0.014170301024324443 pop-kl@10:1.5345421348429984 rhit@10:2.2154903295014243 rndcg@10:1.2390210774133943\n",
      "[INFO] MF-MoRec-Pretrain: No better score in the epoch. Patience: 2 / 10\n",
      "[INFO] MF-MoRec-Pretrain: \n",
      ">> epoch 29\n",
      "Train: 100%|██████████| 1608/1608 [00:10<00:00, 148.17it/s]\n",
      "[INFO] MF-MoRec-Pretrain: epoch 29 training [time: 10.86s, train loss: 296.7733]\n",
      "[INFO] MF-MoRec-Pretrain: one_vs_all\n",
      "Evaluate: 100%|██████████| 122/122 [01:20<00:00,  1.51it/s]\n",
      "[INFO] MF-MoRec-Pretrain: epoch 29 evaluating [time: 80.76s, ndcg@10: 0.014258]\n",
      "[INFO] MF-MoRec-Pretrain: complete scores on valid set: \n",
      "hit@10:0.02522495116718435 min-hit@10:0.01342038447587958 min-ndcg@10:0.006799422029274093 min-rhit@10:0.3822705839680813 min-rndcg@10:0.18015421922707706 ndcg@10:0.014257586865914983 pop-kl@10:1.5134715783462953 rhit@10:2.277717506164142 rndcg@10:1.2563797559812568\n",
      "[INFO] MF-MoRec-Pretrain: Saving best model at epoch 29 to /home/v-huangxu/.unirec/output/Electronics/MF/morec_pretrain_2023-10-28_11-35-22/MF-MoRec-Pretrain.pth\n",
      "[INFO] MF-MoRec-Pretrain: \n",
      ">> epoch 30\n",
      "Train: 100%|██████████| 1608/1608 [00:10<00:00, 148.78it/s]\n",
      "[INFO] MF-MoRec-Pretrain: epoch 30 training [time: 10.81s, train loss: 295.5209]\n",
      "[INFO] MF-MoRec-Pretrain: one_vs_all\n",
      "Evaluate: 100%|██████████| 122/122 [01:20<00:00,  1.51it/s]\n",
      "[INFO] MF-MoRec-Pretrain: epoch 30 evaluating [time: 80.68s, ndcg@10: 0.014329]\n",
      "[INFO] MF-MoRec-Pretrain: complete scores on valid set: \n",
      "hit@10:0.025208940407954145 min-hit@10:0.01342038447587958 min-ndcg@10:0.006796752999028002 min-rhit@10:0.368904606456293 min-rndcg@10:0.18054773758598497 ndcg@10:0.014329252657730277 pop-kl@10:1.4951446526216032 rhit@10:2.2964868391559126 rndcg@10:1.2661605711319532\n",
      "[INFO] MF-MoRec-Pretrain: Saving best model at epoch 30 to /home/v-huangxu/.unirec/output/Electronics/MF/morec_pretrain_2023-10-28_11-35-22/MF-MoRec-Pretrain.pth\n",
      "[INFO] MF-MoRec-Pretrain: \n",
      ">> epoch 31\n",
      "Train: 100%|██████████| 1608/1608 [00:10<00:00, 147.95it/s]\n",
      "[INFO] MF-MoRec-Pretrain: epoch 31 training [time: 10.87s, train loss: 294.7189]\n",
      "[INFO] MF-MoRec-Pretrain: one_vs_all\n",
      "Evaluate: 100%|██████████| 122/122 [01:20<00:00,  1.51it/s]\n",
      "[INFO] MF-MoRec-Pretrain: epoch 31 evaluating [time: 80.62s, ndcg@10: 0.014441]\n",
      "[INFO] MF-MoRec-Pretrain: complete scores on valid set: \n",
      "hit@10:0.025393064139101477 min-hit@10:0.012332245194051506 min-ndcg@10:0.006582182555930428 min-rhit@10:0.3496953210010881 min-rndcg@10:0.1753282787438496 ndcg@10:0.014441392411832074 pop-kl@10:1.5158058499330473 rhit@10:2.313192145121522 rndcg@10:1.2764777893528954\n",
      "[INFO] MF-MoRec-Pretrain: Saving best model at epoch 31 to /home/v-huangxu/.unirec/output/Electronics/MF/morec_pretrain_2023-10-28_11-35-22/MF-MoRec-Pretrain.pth\n",
      "[INFO] MF-MoRec-Pretrain: \n",
      ">> epoch 32\n",
      "Train: 100%|██████████| 1608/1608 [00:10<00:00, 147.93it/s]\n",
      "[INFO] MF-MoRec-Pretrain: epoch 32 training [time: 10.87s, train loss: 293.7883]\n",
      "[INFO] MF-MoRec-Pretrain: one_vs_all\n",
      "Evaluate: 100%|██████████| 122/122 [01:21<00:00,  1.51it/s]\n",
      "[INFO] MF-MoRec-Pretrain: epoch 32 evaluating [time: 81.03s, ndcg@10: 0.014339]\n",
      "[INFO] MF-MoRec-Pretrain: complete scores on valid set: \n",
      "hit@10:0.025393064139101477 min-hit@10:0.014145810663764961 min-ndcg@10:0.007505839326545219 min-rhit@10:0.37868697859992745 min-rndcg@10:0.19227491896368867 ndcg@10:0.014338592998199751 pop-kl@10:1.4927887601437677 rhit@10:2.24622898587851 rndcg@10:1.2510588179146644\n",
      "[INFO] MF-MoRec-Pretrain: No better score in the epoch. Patience: 1 / 10\n",
      "[INFO] MF-MoRec-Pretrain: \n",
      ">> epoch 33\n",
      "Train: 100%|██████████| 1608/1608 [00:10<00:00, 148.03it/s]\n",
      "[INFO] MF-MoRec-Pretrain: epoch 33 training [time: 10.87s, train loss: 292.9582]\n",
      "[INFO] MF-MoRec-Pretrain: one_vs_all\n",
      "Evaluate: 100%|██████████| 122/122 [01:20<00:00,  1.51it/s]\n",
      "[INFO] MF-MoRec-Pretrain: epoch 33 evaluating [time: 80.79s, ndcg@10: 0.014524]\n",
      "[INFO] MF-MoRec-Pretrain: complete scores on valid set: \n",
      "hit@10:0.025745300842165936 min-hit@10:0.01342038447587958 min-ndcg@10:0.007169714107980688 min-rhit@10:0.3720239390642002 min-rndcg@10:0.18580891570026456 ndcg@10:0.014523879958588663 pop-kl@10:1.4828010977148711 rhit@10:2.323567277210285 rndcg@10:1.2846893720279347\n",
      "[INFO] MF-MoRec-Pretrain: Saving best model at epoch 33 to /home/v-huangxu/.unirec/output/Electronics/MF/morec_pretrain_2023-10-28_11-35-22/MF-MoRec-Pretrain.pth\n",
      "[INFO] MF-MoRec-Pretrain: \n",
      ">> epoch 34\n",
      "Train: 100%|██████████| 1608/1608 [00:11<00:00, 142.40it/s]\n",
      "[INFO] MF-MoRec-Pretrain: epoch 34 training [time: 11.30s, train loss: 292.0784]\n",
      "[INFO] MF-MoRec-Pretrain: one_vs_all\n",
      "Evaluate: 100%|██████████| 122/122 [01:20<00:00,  1.51it/s]\n",
      "[INFO] MF-MoRec-Pretrain: epoch 34 evaluating [time: 80.76s, ndcg@10: 0.014410]\n",
      "[INFO] MF-MoRec-Pretrain: complete scores on valid set: \n",
      "hit@10:0.025497134074097795 min-hit@10:0.01342038447587958 min-ndcg@10:0.007381620897397577 min-rhit@10:0.3700362713093943 min-rndcg@10:0.18593014374517763 ndcg@10:0.014409750585316093 pop-kl@10:1.4691385707978841 rhit@10:2.304882641134842 rndcg@10:1.2828380816042642\n",
      "[INFO] MF-MoRec-Pretrain: No better score in the epoch. Patience: 1 / 10\n",
      "[INFO] MF-MoRec-Pretrain: \n",
      ">> epoch 35\n",
      "Train: 100%|██████████| 1608/1608 [00:10<00:00, 148.57it/s]\n",
      "[INFO] MF-MoRec-Pretrain: epoch 35 training [time: 10.83s, train loss: 291.6092]\n",
      "[INFO] MF-MoRec-Pretrain: one_vs_all\n",
      "Evaluate: 100%|██████████| 122/122 [01:20<00:00,  1.51it/s]\n",
      "[INFO] MF-MoRec-Pretrain: epoch 35 evaluating [time: 80.60s, ndcg@10: 0.014436]\n",
      "[INFO] MF-MoRec-Pretrain: complete scores on valid set: \n",
      "hit@10:0.025489128694482693 min-hit@10:0.015596663039535727 min-ndcg@10:0.007694105321794307 min-rhit@10:0.45109176641276744 min-rndcg@10:0.19513167853037924 ndcg@10:0.014436035774988152 pop-kl@10:1.4765029719293168 rhit@10:2.271727560920939 rndcg@10:1.276972360084423\n",
      "[INFO] MF-MoRec-Pretrain: No better score in the epoch. Patience: 2 / 10\n",
      "[INFO] MF-MoRec-Pretrain: \n",
      ">> epoch 36\n",
      "Train: 100%|██████████| 1608/1608 [00:10<00:00, 148.76it/s]\n",
      "[INFO] MF-MoRec-Pretrain: epoch 36 training [time: 10.81s, train loss: 290.8187]\n",
      "[INFO] MF-MoRec-Pretrain: one_vs_all\n",
      "Evaluate: 100%|██████████| 122/122 [01:21<00:00,  1.51it/s]\n",
      "[INFO] MF-MoRec-Pretrain: epoch 36 evaluating [time: 81.06s, ndcg@10: 0.014441]\n",
      "[INFO] MF-MoRec-Pretrain: complete scores on valid set: \n",
      "hit@10:0.025513144833327995 min-hit@10:0.014871236851650345 min-ndcg@10:0.008000082051451668 min-rhit@10:0.47484947406601374 min-rndcg@10:0.20924031392034037 ndcg@10:0.014441446216342208 pop-kl@10:1.4835831603767522 rhit@10:2.27697628806558 rndcg@10:1.2737235245751835\n",
      "[INFO] MF-MoRec-Pretrain: No better score in the epoch. Patience: 3 / 10\n",
      "[INFO] MF-MoRec-Pretrain: \n",
      ">> epoch 37\n",
      "Train: 100%|██████████| 1608/1608 [00:10<00:00, 148.11it/s]\n",
      "[INFO] MF-MoRec-Pretrain: epoch 37 training [time: 10.86s, train loss: 290.3451]\n",
      "[INFO] MF-MoRec-Pretrain: one_vs_all\n",
      "Evaluate: 100%|██████████| 122/122 [01:21<00:00,  1.49it/s]\n",
      "[INFO] MF-MoRec-Pretrain: epoch 37 evaluating [time: 81.81s, ndcg@10: 0.014741]\n",
      "[INFO] MF-MoRec-Pretrain: complete scores on valid set: \n",
      "hit@10:0.02596144609177367 min-hit@10:0.014508523757707652 min-ndcg@10:0.007353106960270664 min-rhit@10:0.4251106274936525 min-rndcg@10:0.19212245390040514 ndcg@10:0.014740676974172591 pop-kl@10:1.4671573060879068 rhit@10:2.3277024560504658 rndcg@10:1.2905451281575793\n",
      "[INFO] MF-MoRec-Pretrain: Saving best model at epoch 37 to /home/v-huangxu/.unirec/output/Electronics/MF/morec_pretrain_2023-10-28_11-35-22/MF-MoRec-Pretrain.pth\n",
      "[INFO] MF-MoRec-Pretrain: \n",
      ">> epoch 38\n",
      "Train: 100%|██████████| 1608/1608 [00:10<00:00, 146.64it/s]\n",
      "[INFO] MF-MoRec-Pretrain: epoch 38 training [time: 10.97s, train loss: 289.9489]\n",
      "[INFO] MF-MoRec-Pretrain: one_vs_all\n",
      "Evaluate: 100%|██████████| 122/122 [01:23<00:00,  1.46it/s]\n",
      "[INFO] MF-MoRec-Pretrain: epoch 38 evaluating [time: 83.37s, ndcg@10: 0.014674]\n",
      "[INFO] MF-MoRec-Pretrain: complete scores on valid set: \n",
      "hit@10:0.025673252425630025 min-hit@10:0.015233949945593036 min-ndcg@10:0.008027967272899615 min-rhit@10:0.4807290533188249 min-rndcg@10:0.2203767163436697 ndcg@10:0.014674345951063567 pop-kl@10:1.2588238816777617 rhit@10:2.30471276697941 rndcg@10:1.2894710613073213\n",
      "[INFO] MF-MoRec-Pretrain: No better score in the epoch. Patience: 1 / 10\n",
      "[INFO] MF-MoRec-Pretrain: \n",
      ">> epoch 39\n",
      "Train: 100%|██████████| 1608/1608 [00:10<00:00, 146.80it/s]\n",
      "[INFO] MF-MoRec-Pretrain: epoch 39 training [time: 10.96s, train loss: 289.3163]\n",
      "[INFO] MF-MoRec-Pretrain: one_vs_all\n",
      "Evaluate: 100%|██████████| 122/122 [01:21<00:00,  1.49it/s]\n",
      "[INFO] MF-MoRec-Pretrain: epoch 39 evaluating [time: 81.76s, ndcg@10: 0.014576]\n",
      "[INFO] MF-MoRec-Pretrain: complete scores on valid set: \n",
      "hit@10:0.025849370777162255 min-hit@10:0.014871236851650345 min-ndcg@10:0.00773742353913674 min-rhit@10:0.42247007616974974 min-rndcg@10:0.20261135128694263 ndcg@10:0.014575922912466415 pop-kl@10:1.4367910253964171 rhit@10:2.3112472381440323 rndcg@10:1.293119218105692\n",
      "[INFO] MF-MoRec-Pretrain: No better score in the epoch. Patience: 2 / 10\n",
      "[INFO] MF-MoRec-Pretrain: \n",
      ">> epoch 40\n",
      "Train: 100%|██████████| 1608/1608 [00:10<00:00, 149.00it/s]\n",
      "[INFO] MF-MoRec-Pretrain: epoch 40 training [time: 10.79s, train loss: 288.9484]\n",
      "[INFO] MF-MoRec-Pretrain: one_vs_all\n",
      "Evaluate: 100%|██████████| 122/122 [01:21<00:00,  1.49it/s]\n",
      "[INFO] MF-MoRec-Pretrain: epoch 40 evaluating [time: 81.71s, ndcg@10: 0.014879]\n",
      "[INFO] MF-MoRec-Pretrain: complete scores on valid set: \n",
      "hit@10:0.026273655896762626 min-hit@10:0.015233949945593036 min-ndcg@10:0.007767146139928877 min-rhit@10:0.4437141820819732 min-rndcg@10:0.21560331611348593 ndcg@10:0.014879033518138182 pop-kl@10:1.4345378370970483 rhit@10:2.3991669601972525 rndcg@10:1.3475758053596092\n",
      "[INFO] MF-MoRec-Pretrain: Saving best model at epoch 40 to /home/v-huangxu/.unirec/output/Electronics/MF/morec_pretrain_2023-10-28_11-35-22/MF-MoRec-Pretrain.pth\n",
      "[INFO] MF-MoRec-Pretrain: \n",
      ">> epoch 41\n",
      "Train: 100%|██████████| 1608/1608 [00:10<00:00, 148.30it/s]\n",
      "[INFO] MF-MoRec-Pretrain: epoch 41 training [time: 10.85s, train loss: 288.4338]\n",
      "[INFO] MF-MoRec-Pretrain: one_vs_all\n",
      "Evaluate: 100%|██████████| 122/122 [01:22<00:00,  1.48it/s]\n",
      "[INFO] MF-MoRec-Pretrain: epoch 41 evaluating [time: 82.70s, ndcg@10: 0.014996]\n",
      "[INFO] MF-MoRec-Pretrain: complete scores on valid set: \n",
      "hit@10:0.02652982804444587 min-hit@10:0.014871236851650345 min-ndcg@10:0.007513940895595306 min-rhit@10:0.4840152339499456 min-rndcg@10:0.21684778297826568 ndcg@10:0.014995995240062187 pop-kl@10:1.4514857959359577 rhit@10:2.449571632136796 rndcg@10:1.3505358858927812\n",
      "[INFO] MF-MoRec-Pretrain: Saving best model at epoch 41 to /home/v-huangxu/.unirec/output/Electronics/MF/morec_pretrain_2023-10-28_11-35-22/MF-MoRec-Pretrain.pth\n",
      "[INFO] MF-MoRec-Pretrain: \n",
      ">> epoch 42\n",
      "Train: 100%|██████████| 1608/1608 [00:10<00:00, 146.49it/s]\n",
      "[INFO] MF-MoRec-Pretrain: epoch 42 training [time: 10.98s, train loss: 288.1048]\n",
      "[INFO] MF-MoRec-Pretrain: one_vs_all\n",
      "Evaluate: 100%|██████████| 122/122 [01:21<00:00,  1.50it/s]\n",
      "[INFO] MF-MoRec-Pretrain: epoch 42 evaluating [time: 81.13s, ndcg@10: 0.014908]\n",
      "[INFO] MF-MoRec-Pretrain: complete scores on valid set: \n",
      "hit@10:0.026449774248294856 min-hit@10:0.014871236851650345 min-ndcg@10:0.007914766400346118 min-rhit@10:0.49007979688066733 min-rndcg@10:0.2350165757682242 ndcg@10:0.014908331947013163 pop-kl@10:1.4488973039062882 rhit@10:2.440043389157513 rndcg@10:1.3396816554256912\n",
      "[INFO] MF-MoRec-Pretrain: No better score in the epoch. Patience: 1 / 10\n",
      "[INFO] MF-MoRec-Pretrain: \n",
      ">> epoch 43\n",
      "Train: 100%|██████████| 1608/1608 [00:10<00:00, 147.72it/s]\n",
      "[INFO] MF-MoRec-Pretrain: epoch 43 training [time: 10.89s, train loss: 287.7014]\n",
      "[INFO] MF-MoRec-Pretrain: one_vs_all\n",
      "Evaluate: 100%|██████████| 122/122 [01:20<00:00,  1.51it/s]\n",
      "[INFO] MF-MoRec-Pretrain: epoch 43 evaluating [time: 80.81s, ndcg@10: 0.014851]\n",
      "[INFO] MF-MoRec-Pretrain: complete scores on valid set: \n",
      "hit@10:0.02636972045214384 min-hit@10:0.014871236851650345 min-ndcg@10:0.008084822003940527 min-rhit@10:0.45174102285092493 min-rndcg@10:0.22237983415272877 ndcg@10:0.01485075246226602 pop-kl@10:1.442843603095477 rhit@10:2.4334222197316597 rndcg@10:1.3449061469646981\n",
      "[INFO] MF-MoRec-Pretrain: No better score in the epoch. Patience: 2 / 10\n",
      "[INFO] MF-MoRec-Pretrain: \n",
      ">> epoch 44\n",
      "Train: 100%|██████████| 1608/1608 [00:10<00:00, 148.48it/s]\n",
      "[INFO] MF-MoRec-Pretrain: epoch 44 training [time: 10.83s, train loss: 287.3407]\n",
      "[INFO] MF-MoRec-Pretrain: one_vs_all\n",
      "Evaluate: 100%|██████████| 122/122 [01:20<00:00,  1.51it/s]\n",
      "[INFO] MF-MoRec-Pretrain: epoch 44 evaluating [time: 80.97s, ndcg@10: 0.014925]\n",
      "[INFO] MF-MoRec-Pretrain: complete scores on valid set: \n",
      "hit@10:0.02654583880367607 min-hit@10:0.015596663039535727 min-ndcg@10:0.008099651446421769 min-rhit@10:0.5256795153034576 min-rndcg@10:0.2311746670286901 ndcg@10:0.01492468188818881 pop-kl@10:1.4305127381167175 rhit@10:2.4188076787601265 rndcg@10:1.3303084164049515\n",
      "[INFO] MF-MoRec-Pretrain: No better score in the epoch. Patience: 3 / 10\n",
      "[INFO] MF-MoRec-Pretrain: \n",
      ">> epoch 45\n",
      "Train: 100%|██████████| 1608/1608 [00:10<00:00, 148.67it/s]\n",
      "[INFO] MF-MoRec-Pretrain: epoch 45 training [time: 10.82s, train loss: 287.1453]\n",
      "[INFO] MF-MoRec-Pretrain: one_vs_all\n",
      "Evaluate: 100%|██████████| 122/122 [01:20<00:00,  1.51it/s]\n",
      "[INFO] MF-MoRec-Pretrain: epoch 45 evaluating [time: 80.87s, ndcg@10: 0.014893]\n",
      "[INFO] MF-MoRec-Pretrain: complete scores on valid set: \n",
      "hit@10:0.02654583880367607 min-hit@10:0.015596663039535727 min-ndcg@10:0.008070823604911305 min-rhit@10:0.5277765687341314 min-rndcg@10:0.23677154677740836 ndcg@10:0.014893087782398843 pop-kl@10:1.4521402138749155 rhit@10:2.4419637196195842 rndcg@10:1.344019320229277\n",
      "[INFO] MF-MoRec-Pretrain: No better score in the epoch. Patience: 4 / 10\n",
      "[INFO] MF-MoRec-Pretrain: \n",
      ">> epoch 46\n",
      "Train: 100%|██████████| 1608/1608 [00:10<00:00, 149.07it/s]\n",
      "[INFO] MF-MoRec-Pretrain: epoch 46 training [time: 10.79s, train loss: 286.7380]\n",
      "[INFO] MF-MoRec-Pretrain: one_vs_all\n",
      "Evaluate: 100%|██████████| 122/122 [01:20<00:00,  1.51it/s]\n",
      "[INFO] MF-MoRec-Pretrain: epoch 46 evaluating [time: 80.77s, ndcg@10: 0.014884]\n",
      "[INFO] MF-MoRec-Pretrain: complete scores on valid set: \n",
      "hit@10:0.02655384418329117 min-hit@10:0.01603206412825651 min-ndcg@10:0.008195054090097532 min-rhit@10:0.5264791601378879 min-rndcg@10:0.24726584553147596 ndcg@10:0.01488399316112765 pop-kl@10:1.4415290025533078 rhit@10:2.4488259310256493 rndcg@10:1.3517815854762565\n",
      "[INFO] MF-MoRec-Pretrain: No better score in the epoch. Patience: 5 / 10\n",
      "[INFO] MF-MoRec-Pretrain: \n",
      ">> epoch 47\n",
      "Train: 100%|██████████| 1608/1608 [00:10<00:00, 148.66it/s]\n",
      "[INFO] MF-MoRec-Pretrain: epoch 47 training [time: 10.82s, train loss: 286.4417]\n",
      "[INFO] MF-MoRec-Pretrain: one_vs_all\n",
      "Evaluate: 100%|██████████| 122/122 [01:20<00:00,  1.51it/s]\n",
      "[INFO] MF-MoRec-Pretrain: epoch 47 evaluating [time: 80.90s, ndcg@10: 0.015024]\n",
      "[INFO] MF-MoRec-Pretrain: complete scores on valid set: \n",
      "hit@10:0.0267219571552083 min-hit@10:0.014487079091620987 min-ndcg@10:0.00812700377355323 min-rhit@10:0.53858769455761 min-rndcg@10:0.25689848269002064 ndcg@10:0.015023644498046305 pop-kl@10:1.441704869662591 rhit@10:2.4392425309808194 rndcg@10:1.3452429481594057\n",
      "[INFO] MF-MoRec-Pretrain: Saving best model at epoch 47 to /home/v-huangxu/.unirec/output/Electronics/MF/morec_pretrain_2023-10-28_11-35-22/MF-MoRec-Pretrain.pth\n",
      "[INFO] MF-MoRec-Pretrain: \n",
      ">> epoch 48\n",
      "Train: 100%|██████████| 1608/1608 [00:10<00:00, 147.44it/s]\n",
      "[INFO] MF-MoRec-Pretrain: epoch 48 training [time: 10.91s, train loss: 286.1886]\n",
      "[INFO] MF-MoRec-Pretrain: one_vs_all\n",
      "Evaluate: 100%|██████████| 122/122 [01:21<00:00,  1.50it/s]\n",
      "[INFO] MF-MoRec-Pretrain: epoch 48 evaluating [time: 81.11s, ndcg@10: 0.015096]\n",
      "[INFO] MF-MoRec-Pretrain: complete scores on valid set: \n",
      "hit@10:0.02689807550674053 min-hit@10:0.016760794315904538 min-ndcg@10:0.008457880972113392 min-rhit@10:0.5269169387014871 min-rndcg@10:0.2374016579972126 ndcg@10:0.015096353594999971 pop-kl@10:1.450300438075486 rhit@10:2.4977309552018956 rndcg@10:1.3805722633074657\n",
      "[INFO] MF-MoRec-Pretrain: Saving best model at epoch 48 to /home/v-huangxu/.unirec/output/Electronics/MF/morec_pretrain_2023-10-28_11-35-22/MF-MoRec-Pretrain.pth\n",
      "[INFO] MF-MoRec-Pretrain: \n",
      ">> epoch 49\n",
      "Train: 100%|██████████| 1608/1608 [00:10<00:00, 148.02it/s]\n",
      "[INFO] MF-MoRec-Pretrain: epoch 49 training [time: 10.87s, train loss: 285.9134]\n",
      "[INFO] MF-MoRec-Pretrain: one_vs_all\n",
      "Evaluate: 100%|██████████| 122/122 [01:21<00:00,  1.50it/s]\n",
      "[INFO] MF-MoRec-Pretrain: epoch 49 evaluating [time: 81.14s, ndcg@10: 0.015029]\n",
      "[INFO] MF-MoRec-Pretrain: complete scores on valid set: \n",
      "hit@10:0.026858048608665024 min-hit@10:0.014145810663764961 min-ndcg@10:0.0074948446258420115 min-rhit@10:0.45939789626405514 min-rndcg@10:0.20606998140049604 ndcg@10:0.015029334629056385 pop-kl@10:1.2225945081524072 rhit@10:2.491542236382849 rndcg@10:1.3573002413995314\n",
      "[INFO] MF-MoRec-Pretrain: No better score in the epoch. Patience: 1 / 10\n",
      "[INFO] MF-MoRec-Pretrain: \n",
      ">> epoch 50\n",
      "Train: 100%|██████████| 1608/1608 [00:10<00:00, 148.46it/s]\n",
      "[INFO] MF-MoRec-Pretrain: epoch 50 training [time: 10.83s, train loss: 285.7913]\n",
      "[INFO] MF-MoRec-Pretrain: one_vs_all\n",
      "Evaluate: 100%|██████████| 122/122 [01:20<00:00,  1.51it/s]\n",
      "[INFO] MF-MoRec-Pretrain: epoch 50 evaluating [time: 81.00s, ndcg@10: 0.015109]\n",
      "[INFO] MF-MoRec-Pretrain: complete scores on valid set: \n",
      "hit@10:0.027002145441736847 min-hit@10:0.015233949945593036 min-ndcg@10:0.007553051353878634 min-rhit@10:0.4991113529198405 min-rndcg@10:0.2144464706042134 ndcg@10:0.015108802146348034 pop-kl@10:1.2334721699651794 rhit@10:2.4467352460853693 rndcg@10:1.3583698100491137\n",
      "[INFO] MF-MoRec-Pretrain: Saving best model at epoch 50 to /home/v-huangxu/.unirec/output/Electronics/MF/morec_pretrain_2023-10-28_11-35-22/MF-MoRec-Pretrain.pth\n",
      "[INFO] MF-MoRec-Pretrain: \n",
      ">> epoch 51\n",
      "Train: 100%|██████████| 1608/1608 [00:10<00:00, 149.15it/s]\n",
      "[INFO] MF-MoRec-Pretrain: epoch 51 training [time: 10.78s, train loss: 285.4682]\n",
      "[INFO] MF-MoRec-Pretrain: one_vs_all\n",
      "Evaluate: 100%|██████████| 122/122 [01:21<00:00,  1.51it/s]\n",
      "[INFO] MF-MoRec-Pretrain: epoch 51 evaluating [time: 81.04s, ndcg@10: 0.015082]\n",
      "[INFO] MF-MoRec-Pretrain: complete scores on valid set: \n",
      "hit@10:0.0267299625348234 min-hit@10:0.014145810663764961 min-ndcg@10:0.007530525493419899 min-rhit@10:0.4288973521944142 min-rndcg@10:0.19569103863962273 ndcg@10:0.015082431125943381 pop-kl@10:1.2386783142477031 rhit@10:2.4458130263537097 rndcg@10:1.3537769780175064\n",
      "[INFO] MF-MoRec-Pretrain: No better score in the epoch. Patience: 1 / 10\n",
      "[INFO] MF-MoRec-Pretrain: \n",
      ">> epoch 52\n",
      "Train: 100%|██████████| 1608/1608 [00:10<00:00, 148.87it/s]\n",
      "[INFO] MF-MoRec-Pretrain: epoch 52 training [time: 10.80s, train loss: 285.3864]\n",
      "[INFO] MF-MoRec-Pretrain: one_vs_all\n",
      "Evaluate: 100%|██████████| 122/122 [01:21<00:00,  1.51it/s]\n",
      "[INFO] MF-MoRec-Pretrain: epoch 52 evaluating [time: 81.02s, ndcg@10: 0.015242]\n",
      "[INFO] MF-MoRec-Pretrain: complete scores on valid set: \n",
      "hit@10:0.02701015082135195 min-hit@10:0.014145810663764961 min-ndcg@10:0.00753433063949573 min-rhit@10:0.39996372869060576 min-rndcg@10:0.189918291498339 ndcg@10:0.01524176007896553 pop-kl@10:1.4236932345934195 rhit@10:2.457795878830574 rndcg@10:1.3800680171782316\n",
      "[INFO] MF-MoRec-Pretrain: Saving best model at epoch 52 to /home/v-huangxu/.unirec/output/Electronics/MF/morec_pretrain_2023-10-28_11-35-22/MF-MoRec-Pretrain.pth\n",
      "[INFO] MF-MoRec-Pretrain: \n",
      ">> epoch 53\n",
      "Train: 100%|██████████| 1608/1608 [00:10<00:00, 147.95it/s]\n",
      "[INFO] MF-MoRec-Pretrain: epoch 53 training [time: 10.87s, train loss: 285.1192]\n",
      "[INFO] MF-MoRec-Pretrain: one_vs_all\n",
      "Evaluate: 100%|██████████| 122/122 [01:21<00:00,  1.50it/s]\n",
      "[INFO] MF-MoRec-Pretrain: epoch 53 evaluating [time: 81.42s, ndcg@10: 0.015174]\n",
      "[INFO] MF-MoRec-Pretrain: complete scores on valid set: \n",
      "hit@10:0.027042172339812355 min-hit@10:0.014508523757707652 min-ndcg@10:0.007670835597152468 min-rhit@10:0.4779361624954661 min-rndcg@10:0.21028109749727322 ndcg@10:0.01517361954417953 pop-kl@10:1.436631010002193 rhit@10:2.5100946235870505 rndcg@10:1.3846495747854284\n",
      "[INFO] MF-MoRec-Pretrain: No better score in the epoch. Patience: 1 / 10\n",
      "[INFO] MF-MoRec-Pretrain: \n",
      ">> epoch 54\n",
      "Train: 100%|██████████| 1608/1608 [00:10<00:00, 148.38it/s]\n",
      "[INFO] MF-MoRec-Pretrain: epoch 54 training [time: 10.84s, train loss: 284.8838]\n",
      "[INFO] MF-MoRec-Pretrain: one_vs_all\n",
      "Evaluate: 100%|██████████| 122/122 [01:21<00:00,  1.49it/s]\n",
      "[INFO] MF-MoRec-Pretrain: epoch 54 evaluating [time: 81.71s, ndcg@10: 0.015261]\n",
      "[INFO] MF-MoRec-Pretrain: complete scores on valid set: \n",
      "hit@10:0.02701015082135195 min-hit@10:0.014871236851650345 min-ndcg@10:0.007844363367523541 min-rhit@10:0.478240841494378 min-rndcg@10:0.2163614410277787 ndcg@10:0.015260503083605275 pop-kl@10:1.4290294630977596 rhit@10:2.5141122034006855 rndcg@10:1.3923124516778598\n",
      "[INFO] MF-MoRec-Pretrain: Saving best model at epoch 54 to /home/v-huangxu/.unirec/output/Electronics/MF/morec_pretrain_2023-10-28_11-35-22/MF-MoRec-Pretrain.pth\n",
      "[INFO] MF-MoRec-Pretrain: \n",
      ">> epoch 55\n",
      "Train: 100%|██████████| 1608/1608 [00:10<00:00, 148.78it/s]\n",
      "[INFO] MF-MoRec-Pretrain: epoch 55 training [time: 10.81s, train loss: 284.5510]\n",
      "[INFO] MF-MoRec-Pretrain: one_vs_all\n",
      "Evaluate: 100%|██████████| 122/122 [01:21<00:00,  1.50it/s]\n",
      "[INFO] MF-MoRec-Pretrain: epoch 55 evaluating [time: 81.45s, ndcg@10: 0.015160]\n",
      "[INFO] MF-MoRec-Pretrain: complete scores on valid set: \n",
      "hit@10:0.027066188478657657 min-hit@10:0.015233949945593036 min-ndcg@10:0.00783037211946528 min-rhit@10:0.5028616943486891 min-rndcg@10:0.2271665764756878 ndcg@10:0.015159501327565886 pop-kl@10:1.435612566012 rhit@10:2.4654772006788566 rndcg@10:1.3723081843983718\n",
      "[INFO] MF-MoRec-Pretrain: No better score in the epoch. Patience: 1 / 10\n",
      "[INFO] MF-MoRec-Pretrain: \n",
      ">> epoch 56\n",
      "Train: 100%|██████████| 1608/1608 [00:10<00:00, 147.63it/s]\n",
      "[INFO] MF-MoRec-Pretrain: epoch 56 training [time: 10.89s, train loss: 284.5301]\n",
      "[INFO] MF-MoRec-Pretrain: one_vs_all\n",
      "Evaluate: 100%|██████████| 122/122 [01:23<00:00,  1.47it/s]\n",
      "[INFO] MF-MoRec-Pretrain: epoch 56 evaluating [time: 83.13s, ndcg@10: 0.015195]\n",
      "[INFO] MF-MoRec-Pretrain: complete scores on valid set: \n",
      "hit@10:0.027290339107880496 min-hit@10:0.015596663039535727 min-ndcg@10:0.007858954010494218 min-rhit@10:0.5214581066376496 min-rndcg@10:0.2235113798376778 ndcg@10:0.015195096770889533 pop-kl@10:1.4438572181070057 rhit@10:2.5097274968779018 rndcg@10:1.391238724398876\n",
      "[INFO] MF-MoRec-Pretrain: No better score in the epoch. Patience: 2 / 10\n",
      "[INFO] MF-MoRec-Pretrain: \n",
      ">> epoch 57\n",
      "Train: 100%|██████████| 1608/1608 [00:10<00:00, 147.09it/s]\n",
      "[INFO] MF-MoRec-Pretrain: epoch 57 training [time: 10.93s, train loss: 284.3163]\n",
      "[INFO] MF-MoRec-Pretrain: one_vs_all\n",
      "Evaluate: 100%|██████████| 122/122 [01:24<00:00,  1.45it/s]\n",
      "[INFO] MF-MoRec-Pretrain: epoch 57 evaluating [time: 84.37s, ndcg@10: 0.015360]\n",
      "[INFO] MF-MoRec-Pretrain: complete scores on valid set: \n",
      "hit@10:0.027410419802107015 min-hit@10:0.015596663039535727 min-ndcg@10:0.00832810165547675 min-rhit@10:0.5393035908596301 min-rndcg@10:0.24917244289957982 ndcg@10:0.015360312379201845 pop-kl@10:1.4361510003140125 rhit@10:2.487990809824202 rndcg@10:1.3721475642369358\n",
      "[INFO] MF-MoRec-Pretrain: Saving best model at epoch 57 to /home/v-huangxu/.unirec/output/Electronics/MF/morec_pretrain_2023-10-28_11-35-22/MF-MoRec-Pretrain.pth\n",
      "[INFO] MF-MoRec-Pretrain: \n",
      ">> epoch 58\n",
      "Train: 100%|██████████| 1608/1608 [00:10<00:00, 147.40it/s]\n",
      "[INFO] MF-MoRec-Pretrain: epoch 58 training [time: 10.91s, train loss: 284.1884]\n",
      "[INFO] MF-MoRec-Pretrain: one_vs_all\n",
      "Evaluate: 100%|██████████| 122/122 [01:21<00:00,  1.49it/s]\n",
      "[INFO] MF-MoRec-Pretrain: epoch 58 evaluating [time: 81.70s, ndcg@10: 0.015309]\n",
      "[INFO] MF-MoRec-Pretrain: complete scores on valid set: \n",
      "hit@10:0.027346376765186205 min-hit@10:0.014878621769772905 min-ndcg@10:0.009133493960012114 min-rhit@10:0.585103415857098 min-rndcg@10:0.2906245251311516 ndcg@10:0.015309442448160161 pop-kl@10:1.450620991450712 rhit@10:2.5023083512120143 rndcg@10:1.3975965067473488\n",
      "[INFO] MF-MoRec-Pretrain: No better score in the epoch. Patience: 1 / 10\n",
      "[INFO] MF-MoRec-Pretrain: \n",
      ">> epoch 59\n",
      "Train: 100%|██████████| 1608/1608 [00:10<00:00, 148.65it/s]\n",
      "[INFO] MF-MoRec-Pretrain: epoch 59 training [time: 10.82s, train loss: 284.0106]\n",
      "[INFO] MF-MoRec-Pretrain: one_vs_all\n",
      "Evaluate: 100%|██████████| 122/122 [01:21<00:00,  1.49it/s]\n",
      "[INFO] MF-MoRec-Pretrain: epoch 59 evaluating [time: 81.67s, ndcg@10: 0.015457]\n",
      "[INFO] MF-MoRec-Pretrain: complete scores on valid set: \n",
      "hit@10:0.027586538153639244 min-hit@10:0.016760794315904538 min-ndcg@10:0.008751775343198829 min-rhit@10:0.5713565469713456 min-rndcg@10:0.25959096238248774 ndcg@10:0.01545715042800188 pop-kl@10:1.2188402048706126 rhit@10:2.5525243363540295 rndcg@10:1.4258015829600486\n",
      "[INFO] MF-MoRec-Pretrain: Saving best model at epoch 59 to /home/v-huangxu/.unirec/output/Electronics/MF/morec_pretrain_2023-10-28_11-35-22/MF-MoRec-Pretrain.pth\n",
      "[INFO] MF-MoRec-Pretrain: \n",
      ">> epoch 60\n",
      "Train: 100%|██████████| 1608/1608 [00:10<00:00, 147.54it/s]\n",
      "[INFO] MF-MoRec-Pretrain: epoch 60 training [time: 10.90s, train loss: 283.7595]\n",
      "[INFO] MF-MoRec-Pretrain: one_vs_all\n",
      "Evaluate: 100%|██████████| 122/122 [01:21<00:00,  1.50it/s]\n",
      "[INFO] MF-MoRec-Pretrain: epoch 60 evaluating [time: 81.59s, ndcg@10: 0.015246]\n",
      "[INFO] MF-MoRec-Pretrain: complete scores on valid set: \n",
      "hit@10:0.02712222613596337 min-hit@10:0.015233949945593036 min-ndcg@10:0.00805343445697646 min-rhit@10:0.4702575262966993 min-rndcg@10:0.21791519396680403 ndcg@10:0.015246220321672398 pop-kl@10:1.4466131451704678 rhit@10:2.4530338787665316 rndcg@10:1.390139506455377\n",
      "[INFO] MF-MoRec-Pretrain: No better score in the epoch. Patience: 1 / 10\n",
      "[INFO] MF-MoRec-Pretrain: \n",
      ">> epoch 61\n",
      "Train: 100%|██████████| 1608/1608 [00:10<00:00, 148.12it/s]\n",
      "[INFO] MF-MoRec-Pretrain: epoch 61 training [time: 10.86s, train loss: 283.6201]\n",
      "[INFO] MF-MoRec-Pretrain: one_vs_all\n",
      "Evaluate: 100%|██████████| 122/122 [01:21<00:00,  1.50it/s]\n",
      "[INFO] MF-MoRec-Pretrain: epoch 61 evaluating [time: 81.16s, ndcg@10: 0.015417]\n",
      "[INFO] MF-MoRec-Pretrain: complete scores on valid set: \n",
      "hit@10:0.027402414422491913 min-hit@10:0.01632208922742111 min-ndcg@10:0.008200663789902838 min-rhit@10:0.49392092854552055 min-rndcg@10:0.22255990820785285 ndcg@10:0.01541739121723418 pop-kl@10:1.4331957306712453 rhit@10:2.4748952095808385 rndcg@10:1.4067150145094711\n",
      "[INFO] MF-MoRec-Pretrain: No better score in the epoch. Patience: 2 / 10\n",
      "[INFO] MF-MoRec-Pretrain: \n",
      ">> epoch 62\n",
      "Train: 100%|██████████| 1608/1608 [00:10<00:00, 148.38it/s]\n",
      "[INFO] MF-MoRec-Pretrain: epoch 62 training [time: 10.84s, train loss: 283.5554]\n",
      "[INFO] MF-MoRec-Pretrain: one_vs_all\n",
      "Evaluate: 100%|██████████| 122/122 [01:23<00:00,  1.47it/s]\n",
      "[INFO] MF-MoRec-Pretrain: epoch 62 evaluating [time: 83.09s, ndcg@10: 0.015443]\n",
      "[INFO] MF-MoRec-Pretrain: complete scores on valid set: \n",
      "hit@10:0.027842710301322487 min-hit@10:0.016684802321363802 min-ndcg@10:0.008602981488439326 min-rhit@10:0.5448059484947407 min-rndcg@10:0.24524473739974903 ndcg@10:0.01544330663784629 pop-kl@10:1.432776824707408 rhit@10:2.5171346344743664 rndcg@10:1.4021315020046539\n",
      "[INFO] MF-MoRec-Pretrain: No better score in the epoch. Patience: 3 / 10\n",
      "[INFO] MF-MoRec-Pretrain: \n",
      ">> epoch 63\n",
      "Train: 100%|██████████| 1608/1608 [00:10<00:00, 148.10it/s]\n",
      "[INFO] MF-MoRec-Pretrain: epoch 63 training [time: 10.86s, train loss: 283.3840]\n",
      "[INFO] MF-MoRec-Pretrain: one_vs_all\n",
      "Evaluate: 100%|██████████| 122/122 [01:22<00:00,  1.47it/s]\n",
      "[INFO] MF-MoRec-Pretrain: epoch 63 evaluating [time: 82.99s, ndcg@10: 0.015251]\n",
      "[INFO] MF-MoRec-Pretrain: complete scores on valid set: \n",
      "hit@10:0.02720227993211438 min-hit@10:0.014871236851650345 min-ndcg@10:0.007991453485103887 min-rhit@10:0.45797606093579973 min-rndcg@10:0.21318675320220623 ndcg@10:0.015250678220635635 pop-kl@10:1.4158099939062003 rhit@10:2.4915050113676394 rndcg@10:1.395513192570047\n",
      "[INFO] MF-MoRec-Pretrain: No better score in the epoch. Patience: 4 / 10\n",
      "[INFO] MF-MoRec-Pretrain: \n",
      ">> epoch 64\n",
      "Train: 100%|██████████| 1608/1608 [00:10<00:00, 148.38it/s]\n",
      "[INFO] MF-MoRec-Pretrain: epoch 64 training [time: 10.84s, train loss: 283.3611]\n",
      "[INFO] MF-MoRec-Pretrain: one_vs_all\n",
      "Evaluate: 100%|██████████| 122/122 [01:21<00:00,  1.50it/s]\n",
      "[INFO] MF-MoRec-Pretrain: epoch 64 evaluating [time: 81.44s, ndcg@10: 0.015426]\n",
      "[INFO] MF-MoRec-Pretrain: complete scores on valid set: \n",
      "hit@10:0.02768260270902046 min-hit@10:0.014871236851650345 min-ndcg@10:0.007630939115188971 min-rhit@10:0.4762966993108452 min-rndcg@10:0.2120811662865026 ndcg@10:0.015425753392857327 pop-kl@10:1.4238650708291485 rhit@10:2.514312978321432 rndcg@10:1.3837089689108317\n",
      "[INFO] MF-MoRec-Pretrain: No better score in the epoch. Patience: 5 / 10\n",
      "[INFO] MF-MoRec-Pretrain: \n",
      ">> epoch 65\n",
      "Train: 100%|██████████| 1608/1608 [00:10<00:00, 147.79it/s]\n",
      "[INFO] MF-MoRec-Pretrain: epoch 65 training [time: 10.88s, train loss: 283.1872]\n",
      "[INFO] MF-MoRec-Pretrain: one_vs_all\n",
      "Evaluate: 100%|██████████| 122/122 [01:21<00:00,  1.49it/s]\n",
      "[INFO] MF-MoRec-Pretrain: epoch 65 evaluating [time: 81.79s, ndcg@10: 0.015507]\n",
      "[INFO] MF-MoRec-Pretrain: complete scores on valid set: \n",
      "hit@10:0.02761055429248455 min-hit@10:0.01644479248238058 min-ndcg@10:0.008625527403967975 min-rhit@10:0.49204899195654445 min-rndcg@10:0.25161525735742274 ndcg@10:0.015507365072237324 pop-kl@10:1.6069835299419428 rhit@10:2.504124291523904 rndcg@10:1.3953237566242562\n",
      "[INFO] MF-MoRec-Pretrain: Saving best model at epoch 65 to /home/v-huangxu/.unirec/output/Electronics/MF/morec_pretrain_2023-10-28_11-35-22/MF-MoRec-Pretrain.pth\n",
      "[INFO] MF-MoRec-Pretrain: \n",
      ">> epoch 66\n",
      "Train: 100%|██████████| 1608/1608 [00:10<00:00, 148.62it/s]\n",
      "[INFO] MF-MoRec-Pretrain: epoch 66 training [time: 10.82s, train loss: 282.9436]\n",
      "[INFO] MF-MoRec-Pretrain: one_vs_all\n",
      "Evaluate: 100%|██████████| 122/122 [01:21<00:00,  1.49it/s]\n",
      "[INFO] MF-MoRec-Pretrain: epoch 66 evaluating [time: 81.85s, ndcg@10: 0.015553]\n",
      "[INFO] MF-MoRec-Pretrain: complete scores on valid set: \n",
      "hit@10:0.027938774856703704 min-hit@10:0.014487079091620987 min-ndcg@10:0.008261397172019524 min-rhit@10:0.5136796197639194 min-rndcg@10:0.26377153888849714 ndcg@10:0.015553200914859634 pop-kl@10:1.4332616422985895 rhit@10:2.501569454673541 rndcg@10:1.386658313442365\n",
      "[INFO] MF-MoRec-Pretrain: Saving best model at epoch 66 to /home/v-huangxu/.unirec/output/Electronics/MF/morec_pretrain_2023-10-28_11-35-22/MF-MoRec-Pretrain.pth\n",
      "[INFO] MF-MoRec-Pretrain: \n",
      ">> epoch 67\n",
      "Train: 100%|██████████| 1608/1608 [00:10<00:00, 148.09it/s]\n",
      "[INFO] MF-MoRec-Pretrain: epoch 67 training [time: 10.86s, train loss: 282.9354]\n",
      "[INFO] MF-MoRec-Pretrain: one_vs_all\n",
      "Evaluate: 100%|██████████| 122/122 [01:21<00:00,  1.50it/s]\n",
      "[INFO] MF-MoRec-Pretrain: epoch 67 evaluating [time: 81.62s, ndcg@10: 0.015503]\n",
      "[INFO] MF-MoRec-Pretrain: complete scores on valid set: \n",
      "hit@10:0.027778667264401678 min-hit@10:0.016942976862816542 min-ndcg@10:0.008560035214752841 min-rhit@10:0.5390958947038547 min-rndcg@10:0.2797889490992573 ndcg@10:0.015502737283190952 pop-kl@10:1.427658835636235 rhit@10:2.4844542732716386 rndcg@10:1.3824371772045383\n",
      "[INFO] MF-MoRec-Pretrain: No better score in the epoch. Patience: 1 / 10\n",
      "[INFO] MF-MoRec-Pretrain: \n",
      ">> epoch 68\n",
      "Train: 100%|██████████| 1608/1608 [00:10<00:00, 148.63it/s]\n",
      "[INFO] MF-MoRec-Pretrain: epoch 68 training [time: 10.82s, train loss: 282.8506]\n",
      "[INFO] MF-MoRec-Pretrain: one_vs_all\n",
      "Evaluate: 100%|██████████| 122/122 [01:21<00:00,  1.50it/s]\n",
      "[INFO] MF-MoRec-Pretrain: epoch 68 evaluating [time: 81.42s, ndcg@10: 0.015520]\n",
      "[INFO] MF-MoRec-Pretrain: complete scores on valid set: \n",
      "hit@10:0.027842710301322487 min-hit@10:0.01632208922742111 min-ndcg@10:0.008348142191403294 min-rhit@10:0.5218969894813204 min-rndcg@10:0.2413146034323163 ndcg@10:0.015520427723260467 pop-kl@10:1.422740911591777 rhit@10:2.552092286016203 rndcg@10:1.3946386795210177\n",
      "[INFO] MF-MoRec-Pretrain: No better score in the epoch. Patience: 2 / 10\n",
      "[INFO] MF-MoRec-Pretrain: \n",
      ">> epoch 69\n",
      "Train: 100%|██████████| 1608/1608 [00:10<00:00, 148.55it/s]\n",
      "[INFO] MF-MoRec-Pretrain: epoch 69 training [time: 10.83s, train loss: 282.7101]\n",
      "[INFO] MF-MoRec-Pretrain: one_vs_all\n",
      "Evaluate: 100%|██████████| 122/122 [01:21<00:00,  1.50it/s]\n",
      "[INFO] MF-MoRec-Pretrain: epoch 69 evaluating [time: 81.22s, ndcg@10: 0.015584]\n",
      "[INFO] MF-MoRec-Pretrain: complete scores on valid set: \n",
      "hit@10:0.02774664574594127 min-hit@10:0.016942976862816542 min-ndcg@10:0.009046994676741091 min-rhit@10:0.5296432675232425 min-rndcg@10:0.28321566384188396 ndcg@10:0.015584232040966185 pop-kl@10:1.4519127215305998 rhit@10:2.5737897467097888 rndcg@10:1.4068944919353394\n",
      "[INFO] MF-MoRec-Pretrain: Saving best model at epoch 69 to /home/v-huangxu/.unirec/output/Electronics/MF/morec_pretrain_2023-10-28_11-35-22/MF-MoRec-Pretrain.pth\n",
      "[INFO] MF-MoRec-Pretrain: \n",
      ">> epoch 70\n",
      "Train: 100%|██████████| 1608/1608 [00:10<00:00, 148.66it/s]\n",
      "[INFO] MF-MoRec-Pretrain: epoch 70 training [time: 10.82s, train loss: 282.5086]\n",
      "[INFO] MF-MoRec-Pretrain: one_vs_all\n",
      "Evaluate: 100%|██████████| 122/122 [01:21<00:00,  1.50it/s]\n",
      "[INFO] MF-MoRec-Pretrain: epoch 70 evaluating [time: 81.10s, ndcg@10: 0.015644]\n",
      "[INFO] MF-MoRec-Pretrain: complete scores on valid set: \n",
      "hit@10:0.02802683403246982 min-hit@10:0.017410228509249184 min-ndcg@10:0.008991089024493129 min-rhit@10:0.5582999059855844 min-rndcg@10:0.26860622998448325 ndcg@10:0.01564365266026637 pop-kl@10:1.4301173505394287 rhit@10:2.5833614589003813 rndcg@10:1.4280187599566725\n",
      "[INFO] MF-MoRec-Pretrain: Saving best model at epoch 70 to /home/v-huangxu/.unirec/output/Electronics/MF/morec_pretrain_2023-10-28_11-35-22/MF-MoRec-Pretrain.pth\n",
      "[INFO] MF-MoRec-Pretrain: \n",
      ">> epoch 71\n",
      "Train: 100%|██████████| 1608/1608 [00:10<00:00, 147.90it/s]\n",
      "[INFO] MF-MoRec-Pretrain: epoch 71 training [time: 10.87s, train loss: 282.3853]\n",
      "[INFO] MF-MoRec-Pretrain: one_vs_all\n",
      "Evaluate: 100%|██████████| 122/122 [01:21<00:00,  1.50it/s]\n",
      "[INFO] MF-MoRec-Pretrain: epoch 71 evaluating [time: 81.12s, ndcg@10: 0.015528]\n",
      "[INFO] MF-MoRec-Pretrain: complete scores on valid set: \n",
      "hit@10:0.027842710301322487 min-hit@10:0.015233949945593036 min-ndcg@10:0.008565230788464507 min-rhit@10:0.5010083424011608 min-rndcg@10:0.24498162367169019 ndcg@10:0.01552827376668798 pop-kl@10:1.4448156052790644 rhit@10:2.562051538633962 rndcg@10:1.4126849051264752\n",
      "[INFO] MF-MoRec-Pretrain: No better score in the epoch. Patience: 1 / 10\n",
      "[INFO] MF-MoRec-Pretrain: \n",
      ">> epoch 72\n",
      "Train: 100%|██████████| 1608/1608 [00:10<00:00, 148.33it/s]\n",
      "[INFO] MF-MoRec-Pretrain: epoch 72 training [time: 10.84s, train loss: 282.3134]\n",
      "[INFO] MF-MoRec-Pretrain: one_vs_all\n",
      "Evaluate: 100%|██████████| 122/122 [01:21<00:00,  1.49it/s]\n",
      "[INFO] MF-MoRec-Pretrain: epoch 72 evaluating [time: 81.62s, ndcg@10: 0.015540]\n",
      "[INFO] MF-MoRec-Pretrain: complete scores on valid set: \n",
      "hit@10:0.027890742579013098 min-hit@10:0.015596663039535727 min-ndcg@10:0.008614729617671034 min-rhit@10:0.5283532825535002 min-rndcg@10:0.24802841994998326 ndcg@10:0.015539989598235566 pop-kl@10:1.4458123197898198 rhit@10:2.559967418104966 rndcg@10:1.3993583440412003\n",
      "[INFO] MF-MoRec-Pretrain: No better score in the epoch. Patience: 2 / 10\n",
      "[INFO] MF-MoRec-Pretrain: \n",
      ">> epoch 73\n",
      "Train: 100%|██████████| 1608/1608 [00:10<00:00, 149.16it/s]\n",
      "[INFO] MF-MoRec-Pretrain: epoch 73 training [time: 10.78s, train loss: 282.3322]\n",
      "[INFO] MF-MoRec-Pretrain: one_vs_all\n",
      "Evaluate: 100%|██████████| 122/122 [01:21<00:00,  1.49it/s]\n",
      "[INFO] MF-MoRec-Pretrain: epoch 73 evaluating [time: 81.81s, ndcg@10: 0.015522]\n",
      "[INFO] MF-MoRec-Pretrain: complete scores on valid set: \n",
      "hit@10:0.0278987479586282 min-hit@10:0.014871236851650345 min-ndcg@10:0.007847528983516332 min-rhit@10:0.48351468988030477 min-rndcg@10:0.21415829079642273 ndcg@10:0.015521796137019173 pop-kl@10:1.44080417126384 rhit@10:2.5860077171859492 rndcg@10:1.4271602556506326\n",
      "[INFO] MF-MoRec-Pretrain: No better score in the epoch. Patience: 3 / 10\n",
      "[INFO] MF-MoRec-Pretrain: \n",
      ">> epoch 74\n",
      "Train: 100%|██████████| 1608/1608 [00:10<00:00, 149.15it/s]\n",
      "[INFO] MF-MoRec-Pretrain: epoch 74 training [time: 10.78s, train loss: 282.1956]\n",
      "[INFO] MF-MoRec-Pretrain: one_vs_all\n",
      "Evaluate: 100%|██████████| 122/122 [01:23<00:00,  1.47it/s]\n",
      "[INFO] MF-MoRec-Pretrain: epoch 74 evaluating [time: 83.10s, ndcg@10: 0.015698]\n",
      "[INFO] MF-MoRec-Pretrain: complete scores on valid set: \n",
      "hit@10:0.027954785615933907 min-hit@10:0.014871236851650345 min-ndcg@10:0.007860990854252676 min-rhit@10:0.5235364526659412 min-rndcg@10:0.22877740768318713 ndcg@10:0.015698427735559562 pop-kl@10:1.4468608244665995 rhit@10:2.5654258061417274 rndcg@10:1.4462458922277144\n",
      "[INFO] MF-MoRec-Pretrain: Saving best model at epoch 74 to /home/v-huangxu/.unirec/output/Electronics/MF/morec_pretrain_2023-10-28_11-35-22/MF-MoRec-Pretrain.pth\n",
      "[INFO] MF-MoRec-Pretrain: \n",
      ">> epoch 75\n",
      "Train: 100%|██████████| 1608/1608 [00:10<00:00, 148.08it/s]\n",
      "[INFO] MF-MoRec-Pretrain: epoch 75 training [time: 10.86s, train loss: 282.1268]\n",
      "[INFO] MF-MoRec-Pretrain: one_vs_all\n",
      "Evaluate: 100%|██████████| 122/122 [01:23<00:00,  1.46it/s]\n",
      "[INFO] MF-MoRec-Pretrain: epoch 75 evaluating [time: 83.54s, ndcg@10: 0.015532]\n",
      "[INFO] MF-MoRec-Pretrain: complete scores on valid set: \n",
      "hit@10:0.027778667264401678 min-hit@10:0.01632208922742111 min-ndcg@10:0.008375167351094868 min-rhit@10:0.5326296699310845 min-rndcg@10:0.22944205853066407 ndcg@10:0.015531968711145303 pop-kl@10:1.4440460367265957 rhit@10:2.558681113708413 rndcg@10:1.416976237326156\n",
      "[INFO] MF-MoRec-Pretrain: No better score in the epoch. Patience: 1 / 10\n",
      "[INFO] MF-MoRec-Pretrain: \n",
      ">> epoch 76\n",
      "Train: 100%|██████████| 1608/1608 [00:10<00:00, 147.71it/s]\n",
      "[INFO] MF-MoRec-Pretrain: epoch 76 training [time: 10.89s, train loss: 282.1862]\n",
      "[INFO] MF-MoRec-Pretrain: one_vs_all\n",
      "Evaluate: 100%|██████████| 122/122 [01:21<00:00,  1.50it/s]\n",
      "[INFO] MF-MoRec-Pretrain: epoch 76 evaluating [time: 81.38s, ndcg@10: 0.015580]\n",
      "[INFO] MF-MoRec-Pretrain: complete scores on valid set: \n",
      "hit@10:0.02767459732940536 min-hit@10:0.01632208922742111 min-ndcg@10:0.008034527981810601 min-rhit@10:0.5450489662676823 min-rndcg@10:0.23415632054080965 ndcg@10:0.015580064779731322 pop-kl@10:1.4349190267448868 rhit@10:2.509368615709757 rndcg@10:1.4151179330201178\n",
      "[INFO] MF-MoRec-Pretrain: No better score in the epoch. Patience: 2 / 10\n",
      "[INFO] MF-MoRec-Pretrain: \n",
      ">> epoch 77\n",
      "Train: 100%|██████████| 1608/1608 [00:10<00:00, 148.01it/s]\n",
      "[INFO] MF-MoRec-Pretrain: epoch 77 training [time: 10.87s, train loss: 281.9517]\n",
      "[INFO] MF-MoRec-Pretrain: one_vs_all\n",
      "Evaluate: 100%|██████████| 122/122 [01:24<00:00,  1.45it/s]\n",
      "[INFO] MF-MoRec-Pretrain: epoch 77 evaluating [time: 84.25s, ndcg@10: 0.015644]\n",
      "[INFO] MF-MoRec-Pretrain: complete scores on valid set: \n",
      "hit@10:0.027698613468250664 min-hit@10:0.01644479248238058 min-ndcg@10:0.008719158671430263 min-rhit@10:0.5409406664577456 min-rndcg@10:0.2333469047494926 ndcg@10:0.015643751736247862 pop-kl@10:1.4451137185249876 rhit@10:2.529613420218386 rndcg@10:1.4341283438857169\n",
      "[INFO] MF-MoRec-Pretrain: No better score in the epoch. Patience: 3 / 10\n",
      "[INFO] MF-MoRec-Pretrain: \n",
      ">> epoch 78\n",
      "Train: 100%|██████████| 1608/1608 [00:10<00:00, 147.68it/s]\n",
      "[INFO] MF-MoRec-Pretrain: epoch 78 training [time: 10.89s, train loss: 281.9082]\n",
      "[INFO] MF-MoRec-Pretrain: one_vs_all\n",
      "Evaluate: 100%|██████████| 122/122 [01:23<00:00,  1.47it/s]\n",
      "[INFO] MF-MoRec-Pretrain: epoch 78 evaluating [time: 83.12s, ndcg@10: 0.015602]\n",
      "[INFO] MF-MoRec-Pretrain: complete scores on valid set: \n",
      "hit@10:0.027914758717858403 min-hit@10:0.01605324980422866 min-ndcg@10:0.008126603995824202 min-rhit@10:0.5282009430540443 min-rndcg@10:0.22445749292578945 ndcg@10:0.015601905289676365 pop-kl@10:1.45041675688576 rhit@10:2.541296791443851 rndcg@10:1.4281760245408845\n",
      "[INFO] MF-MoRec-Pretrain: No better score in the epoch. Patience: 4 / 10\n",
      "[INFO] MF-MoRec-Pretrain: \n",
      ">> epoch 79\n",
      "Train: 100%|██████████| 1608/1608 [00:10<00:00, 149.22it/s]\n",
      "[INFO] MF-MoRec-Pretrain: epoch 79 training [time: 10.78s, train loss: 281.7454]\n",
      "[INFO] MF-MoRec-Pretrain: one_vs_all\n",
      "Evaluate: 100%|██████████| 122/122 [01:21<00:00,  1.50it/s]\n",
      "[INFO] MF-MoRec-Pretrain: epoch 79 evaluating [time: 81.53s, ndcg@10: 0.015660]\n",
      "[INFO] MF-MoRec-Pretrain: complete scores on valid set: \n",
      "hit@10:0.02809087706939063 min-hit@10:0.018010963194988253 min-ndcg@10:0.008515379329019085 min-rhit@10:0.5657035412096522 min-rndcg@10:0.2585236422870319 ndcg@10:0.015660390848448426 pop-kl@10:1.4412986973483757 rhit@10:2.5216167664670657 rndcg@10:1.418548921041926\n",
      "[INFO] MF-MoRec-Pretrain: No better score in the epoch. Patience: 5 / 10\n",
      "[INFO] MF-MoRec-Pretrain: \n",
      ">> epoch 80\n",
      "Train: 100%|██████████| 1608/1608 [00:10<00:00, 148.10it/s]\n",
      "[INFO] MF-MoRec-Pretrain: epoch 80 training [time: 10.86s, train loss: 281.7928]\n",
      "[INFO] MF-MoRec-Pretrain: one_vs_all\n",
      "Evaluate: 100%|██████████| 122/122 [01:21<00:00,  1.50it/s]\n",
      "[INFO] MF-MoRec-Pretrain: epoch 80 evaluating [time: 81.40s, ndcg@10: 0.015853]\n",
      "[INFO] MF-MoRec-Pretrain: complete scores on valid set: \n",
      "hit@10:0.028226968522847354 min-hit@10:0.018010963194988253 min-ndcg@10:0.009588962031888805 min-rhit@10:0.5446411783140082 min-rndcg@10:0.27312923683920476 ndcg@10:0.015853226085775617 pop-kl@10:1.4318585098193302 rhit@10:2.5707144000768514 rndcg@10:1.4293092277158326\n",
      "[INFO] MF-MoRec-Pretrain: Saving best model at epoch 80 to /home/v-huangxu/.unirec/output/Electronics/MF/morec_pretrain_2023-10-28_11-35-22/MF-MoRec-Pretrain.pth\n",
      "[INFO] MF-MoRec-Pretrain: \n",
      ">> epoch 81\n",
      "Train: 100%|██████████| 1608/1608 [00:10<00:00, 148.76it/s]\n",
      "[INFO] MF-MoRec-Pretrain: epoch 81 training [time: 10.81s, train loss: 281.5458]\n",
      "[INFO] MF-MoRec-Pretrain: one_vs_all\n",
      "Evaluate: 100%|██████████| 122/122 [01:21<00:00,  1.50it/s]\n",
      "[INFO] MF-MoRec-Pretrain: epoch 81 evaluating [time: 81.22s, ndcg@10: 0.015829]\n",
      "[INFO] MF-MoRec-Pretrain: complete scores on valid set: \n",
      "hit@10:0.028018828652854717 min-hit@10:0.016836335160532498 min-ndcg@10:0.009564054168308262 min-rhit@10:0.5115778019586508 min-rndcg@10:0.24742696989818266 ndcg@10:0.015829497503056865 pop-kl@10:1.425198703827928 rhit@10:2.581969483492907 rndcg@10:1.4443500305717791\n",
      "[INFO] MF-MoRec-Pretrain: No better score in the epoch. Patience: 1 / 10\n",
      "[INFO] MF-MoRec-Pretrain: \n",
      ">> epoch 82\n",
      "Train: 100%|██████████| 1608/1608 [00:10<00:00, 148.96it/s]\n",
      "[INFO] MF-MoRec-Pretrain: epoch 82 training [time: 10.80s, train loss: 281.6072]\n",
      "[INFO] MF-MoRec-Pretrain: one_vs_all\n",
      "Evaluate: 100%|██████████| 122/122 [01:22<00:00,  1.49it/s]\n",
      "[INFO] MF-MoRec-Pretrain: epoch 82 evaluating [time: 82.05s, ndcg@10: 0.015793]\n",
      "[INFO] MF-MoRec-Pretrain: complete scores on valid set: \n",
      "hit@10:0.028066860930545327 min-hit@10:0.017047515415306493 min-ndcg@10:0.009161864933167393 min-rhit@10:0.5483822996010156 min-rndcg@10:0.2524901042566124 ndcg@10:0.0157933045018029 pop-kl@10:1.4268801373169824 rhit@10:2.5712718947132474 rndcg@10:1.4299671605814945\n",
      "[INFO] MF-MoRec-Pretrain: No better score in the epoch. Patience: 2 / 10\n",
      "[INFO] MF-MoRec-Pretrain: \n",
      ">> epoch 83\n",
      "Train: 100%|██████████| 1608/1608 [00:10<00:00, 148.05it/s]\n",
      "[INFO] MF-MoRec-Pretrain: epoch 83 training [time: 10.86s, train loss: 281.3649]\n",
      "[INFO] MF-MoRec-Pretrain: one_vs_all\n",
      "Evaluate: 100%|██████████| 122/122 [01:21<00:00,  1.50it/s]\n",
      "[INFO] MF-MoRec-Pretrain: epoch 83 evaluating [time: 81.36s, ndcg@10: 0.015798]\n",
      "[INFO] MF-MoRec-Pretrain: complete scores on valid set: \n",
      "hit@10:0.028170930865541646 min-hit@10:0.017772941603191875 min-ndcg@10:0.009250171118200184 min-rhit@10:0.5459992745738121 min-rndcg@10:0.24972632921526705 ndcg@10:0.015797646354984637 pop-kl@10:1.4275164469759198 rhit@10:2.5467809568029716 rndcg@10:1.4309991234997663\n",
      "[INFO] MF-MoRec-Pretrain: No better score in the epoch. Patience: 3 / 10\n",
      "[INFO] MF-MoRec-Pretrain: \n",
      ">> epoch 84\n",
      "Train: 100%|██████████| 1608/1608 [00:10<00:00, 148.61it/s]\n",
      "[INFO] MF-MoRec-Pretrain: epoch 84 training [time: 10.82s, train loss: 281.3379]\n",
      "[INFO] MF-MoRec-Pretrain: one_vs_all\n",
      "Evaluate: 100%|██████████| 122/122 [01:21<00:00,  1.50it/s]\n",
      "[INFO] MF-MoRec-Pretrain: epoch 84 evaluating [time: 81.49s, ndcg@10: 0.015746]\n",
      "[INFO] MF-MoRec-Pretrain: complete scores on valid set: \n",
      "hit@10:0.028154920106311442 min-hit@10:0.01644479248238058 min-ndcg@10:0.009020148579736516 min-rhit@10:0.5676240467982868 min-rndcg@10:0.2725325122378977 ndcg@10:0.015746164411115492 pop-kl@10:1.4350857240802413 rhit@10:2.5433892375676455 rndcg@10:1.4113159860119369\n",
      "[INFO] MF-MoRec-Pretrain: No better score in the epoch. Patience: 4 / 10\n",
      "[INFO] MF-MoRec-Pretrain: \n",
      ">> epoch 85\n",
      "Train: 100%|██████████| 1608/1608 [00:10<00:00, 148.48it/s]\n",
      "[INFO] MF-MoRec-Pretrain: epoch 85 training [time: 10.83s, train loss: 281.3065]\n",
      "[INFO] MF-MoRec-Pretrain: one_vs_all\n",
      "Evaluate: 100%|██████████| 122/122 [01:21<00:00,  1.50it/s]\n",
      "[INFO] MF-MoRec-Pretrain: epoch 85 evaluating [time: 81.17s, ndcg@10: 0.015762]\n",
      "[INFO] MF-MoRec-Pretrain: complete scores on valid set: \n",
      "hit@10:0.028355054596688974 min-hit@10:0.017047515415306493 min-ndcg@10:0.008474643281498963 min-rhit@10:0.5267428364163946 min-rndcg@10:0.22999469661668287 ndcg@10:0.015761855048158118 pop-kl@10:1.4405711870901412 rhit@10:2.6138801274456442 rndcg@10:1.4318052626487177\n",
      "[INFO] MF-MoRec-Pretrain: No better score in the epoch. Patience: 5 / 10\n",
      "[INFO] MF-MoRec-Pretrain: \n",
      ">> epoch 86\n",
      "Train: 100%|██████████| 1608/1608 [00:10<00:00, 148.90it/s]\n",
      "[INFO] MF-MoRec-Pretrain: epoch 86 training [time: 10.80s, train loss: 281.1544]\n",
      "[INFO] MF-MoRec-Pretrain: one_vs_all\n",
      "Evaluate: 100%|██████████| 122/122 [01:20<00:00,  1.51it/s]\n",
      "[INFO] MF-MoRec-Pretrain: epoch 86 evaluating [time: 80.88s, ndcg@10: 0.015743]\n",
      "[INFO] MF-MoRec-Pretrain: complete scores on valid set: \n",
      "hit@10:0.028122898587851036 min-hit@10:0.018135654697134566 min-ndcg@10:0.008931587723994895 min-rhit@10:0.5886388801838504 min-rndcg@10:0.2610731777998279 ndcg@10:0.015742545837887232 pop-kl@10:1.4428449027085852 rhit@10:2.548850587594864 rndcg@10:1.413287921204281\n",
      "[INFO] MF-MoRec-Pretrain: No better score in the epoch. Patience: 6 / 10\n",
      "[INFO] MF-MoRec-Pretrain: \n",
      ">> epoch 87\n",
      "Train: 100%|██████████| 1608/1608 [00:10<00:00, 148.15it/s]\n",
      "[INFO] MF-MoRec-Pretrain: epoch 87 training [time: 10.86s, train loss: 281.0364]\n",
      "[INFO] MF-MoRec-Pretrain: one_vs_all\n",
      "Evaluate: 100%|██████████| 122/122 [01:21<00:00,  1.50it/s]\n",
      "[INFO] MF-MoRec-Pretrain: epoch 87 evaluating [time: 81.41s, ndcg@10: 0.015673]\n",
      "[INFO] MF-MoRec-Pretrain: complete scores on valid set: \n",
      "hit@10:0.027930769477088602 min-hit@10:0.017047515415306493 min-ndcg@10:0.00904591476595886 min-rhit@10:0.5259630032644178 min-rndcg@10:0.24108042629834478 ndcg@10:0.015672735733197198 pop-kl@10:1.4221275672471254 rhit@10:2.5594790098946487 rndcg@10:1.4303920865708646\n",
      "[INFO] MF-MoRec-Pretrain: No better score in the epoch. Patience: 7 / 10\n",
      "[INFO] MF-MoRec-Pretrain: \n",
      ">> epoch 88\n",
      "Train: 100%|██████████| 1608/1608 [00:10<00:00, 148.39it/s]\n",
      "[INFO] MF-MoRec-Pretrain: epoch 88 training [time: 10.84s, train loss: 281.0338]\n",
      "[INFO] MF-MoRec-Pretrain: one_vs_all\n",
      "Evaluate: 100%|██████████| 122/122 [01:21<00:00,  1.50it/s]\n",
      "[INFO] MF-MoRec-Pretrain: epoch 88 evaluating [time: 81.43s, ndcg@10: 0.015590]\n",
      "[INFO] MF-MoRec-Pretrain: complete scores on valid set: \n",
      "hit@10:0.027810688782862084 min-hit@10:0.015596663039535727 min-ndcg@10:0.008250328724556133 min-rhit@10:0.4397896264055132 min-rndcg@10:0.21456466072894398 ndcg@10:0.015589867111020242 pop-kl@10:1.4126159243026581 rhit@10:2.570194130455666 rndcg@10:1.4275070832318346\n",
      "[INFO] MF-MoRec-Pretrain: No better score in the epoch. Patience: 8 / 10\n",
      "[INFO] MF-MoRec-Pretrain: \n",
      ">> epoch 89\n",
      "Train: 100%|██████████| 1608/1608 [00:10<00:00, 148.51it/s]\n",
      "[INFO] MF-MoRec-Pretrain: epoch 89 training [time: 10.83s, train loss: 281.1170]\n",
      "[INFO] MF-MoRec-Pretrain: one_vs_all\n",
      "Evaluate: 100%|██████████| 122/122 [01:21<00:00,  1.50it/s]\n",
      "[INFO] MF-MoRec-Pretrain: epoch 89 evaluating [time: 81.53s, ndcg@10: 0.015645]\n",
      "[INFO] MF-MoRec-Pretrain: complete scores on valid set: \n",
      "hit@10:0.027938774856703704 min-hit@10:0.017047515415306493 min-ndcg@10:0.008480314172303407 min-rhit@10:0.5518150005223024 min-rndcg@10:0.2565228920497423 ndcg@10:0.01564451274660128 pop-kl@10:1.4099207551798854 rhit@10:2.5892946460021142 rndcg@10:1.4395632875338122\n",
      "[INFO] MF-MoRec-Pretrain: No better score in the epoch. Patience: 9 / 10\n",
      "[INFO] MF-MoRec-Pretrain: \n",
      ">> epoch 90\n",
      "Train: 100%|██████████| 1608/1608 [00:10<00:00, 149.34it/s]\n",
      "[INFO] MF-MoRec-Pretrain: epoch 90 training [time: 10.77s, train loss: 281.0448]\n",
      "[INFO] MF-MoRec-Pretrain: one_vs_all\n",
      "Evaluate: 100%|██████████| 122/122 [01:21<00:00,  1.50it/s]\n",
      "[INFO] MF-MoRec-Pretrain: epoch 90 evaluating [time: 81.30s, ndcg@10: 0.015606]\n",
      "[INFO] MF-MoRec-Pretrain: complete scores on valid set: \n",
      "hit@10:0.02773063498671107 min-hit@10:0.014508523757707652 min-ndcg@10:0.007723686567143914 min-rhit@10:0.4819151251360174 min-rndcg@10:0.2205925199967787 ndcg@10:0.015606106581693655 pop-kl@10:1.4312999482845297 rhit@10:2.5073824009734538 rndcg@10:1.409422658382497\n",
      "[INFO] MF-MoRec-Pretrain: No better score in the epoch. Patience: 10 / 10\n",
      "[INFO] MF-MoRec-Pretrain: \n",
      ">> epoch 91\n",
      "Train: 100%|██████████| 1608/1608 [00:10<00:00, 148.89it/s]\n",
      "[INFO] MF-MoRec-Pretrain: epoch 91 training [time: 10.80s, train loss: 280.9309]\n",
      "[INFO] MF-MoRec-Pretrain: one_vs_all\n",
      "Evaluate: 100%|██████████| 122/122 [01:21<00:00,  1.50it/s]\n",
      "[INFO] MF-MoRec-Pretrain: epoch 91 evaluating [time: 81.31s, ndcg@10: 0.015778]\n",
      "[INFO] MF-MoRec-Pretrain: complete scores on valid set: \n",
      "hit@10:0.028154920106311442 min-hit@10:0.017410228509249184 min-ndcg@10:0.00879099554597424 min-rhit@10:0.5952047425049619 min-rndcg@10:0.26309466415569127 ndcg@10:0.015777613341133073 pop-kl@10:1.433111380666879 rhit@10:2.564445147138877 rndcg@10:1.4295126935149705\n",
      "[INFO] MF-MoRec-Pretrain: No better score in the epoch. Patience: 11 / 10\n",
      "[INFO] MF-MoRec-Pretrain: Finished training, best eval result in epoch 80\n",
      "[INFO] MF-MoRec-Pretrain: Constructing dataset of task type: test\n",
      "[DEBUG] MF-MoRec-Pretrain: loading test at 28/10/2023 13:57:48\n",
      "[DEBUG] MF-MoRec-Pretrain: Finished loading test at 28/10/2023 13:57:48\n",
      "[INFO] MF-MoRec-Pretrain: Finished initializing <class 'unirec.data.dataset.basedataset.BaseDataset'>\n",
      "[INFO] MF-MoRec-Pretrain: one_vs_all\n",
      "[INFO] MF-MoRec-Pretrain: Loading model from /home/v-huangxu/.unirec/output/Electronics/MF/morec_pretrain_2023-10-28_11-35-22/MF-MoRec-Pretrain.pth. The best epoch was 80\n",
      "Evaluate: 100%|██████████| 1250/1250 [02:12<00:00,  9.42it/s]\n",
      "[INFO] MF-MoRec-Pretrain: best valid : {'hit@10': 0.028226968522847354, 'ndcg@10': 0.015853226085775617, 'rhit@10': 2.5707144000768514, 'rndcg@10': 1.4293092277158326, 'pop-kl@10': 1.4318585098193302, 'min-hit@10': 0.018010963194988253, 'min-ndcg@10': 0.009588962031888805, 'min-rhit@10': 0.5446411783140082, 'min-rndcg@10': 0.27312923683920476}\n",
      "[INFO] MF-MoRec-Pretrain: test result: {'hit@10': 0.016234909859425533, 'ndcg@10': 0.008779203758959964, 'rhit@10': 1.3542233180697432, 'rndcg@10': 0.6982034604612465, 'pop-kl@10': 1.4300518294507298, 'min-hit@10': 0.009118371000223764, 'min-ndcg@10': 0.00516860580638276, 'min-rhit@10': 0.21157921235175656, 'min-rndcg@10': 0.12419234342959445}\n",
      "[INFO] MF-MoRec-Pretrain: Saving test result to /home/v-huangxu/.unirec/output/Electronics/MF/result_MF-MoRec-Pretrain.2023-10-28_113522.7.tsv ...\n",
      "[INFO] MF-MoRec-Pretrain: Mission complete. Time elapsed: 144.64 minutes.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logger close successfully.\n",
      "{'hit@10': 0.016234909859425533, 'ndcg@10': 0.008779203758959964, 'rhit@10': 1.3542233180697432, 'rndcg@10': 0.6982034604612465, 'pop-kl@10': 1.4300518294507298, 'min-hit@10': 0.009118371000223764, 'min-ndcg@10': 0.00516860580638276, 'min-rhit@10': 0.21157921235175656, 'min-rndcg@10': 0.12419234342959445}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for arg in sys.argv:  # arguments conflict in notebooks, this is only required in notebooks\n",
    "    if \"-f\" in arg:\n",
    "        sys.argv.remove(arg)\n",
    "\n",
    "pretrain_config = deepcopy(GLOBAL_CONF)\n",
    "\n",
    "pretrain_config['checkpoint_dir'] = 'morec_pretrain_' + pretrain_config['checkpoint_dir']\n",
    "pretrain_config['exp_name'] = \"MoRec-Pretrain\"\n",
    "\n",
    "pretrain_result = main.run(pretrain_config)\n",
    "\n",
    "print(pretrain_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MoRec fine-tuning stage: multi-objective model tuning\n",
    "\n",
    "In this stage, the pretrained model is loaded and then trained successively toward a multi-objective model. \n",
    "\n",
    "Here we only need to set parameters for MoRec. There are several important arguments here.\n",
    "\n",
    "- enable_morec: to enable MoRec finetuning\n",
    "- model_file: the checkpoint file path of pretrained model, which is set as the output path of pretraining stage\n",
    "- morec_objectives: the objectives to be optimized in MoRec\n",
    "- morec_ngroup: group items according to weight. If -1, not use group\n",
    "- morec_alpha: the learning rate to update sampling weight with signed SGD in MoRec data sampler\n",
    "- morec_lambda: the coef $\\lambda$ used in loss synthesis\n",
    "- morec_expect_loss: expected loss for accuracy, used in PID-based objective coordinatoor\n",
    "- morec_beta_min, morec_beta_max: the range of output of PID\n",
    "- morec_K_p, morec_K_i: the coef of proportional and integral part of PID\n",
    "- morec_objective_controller: MoRec objective coordinator(controller), optional [\"Static\", \"PID\"]\n",
    "- morec_objective_weights: Weight to set objective preference. For PID controller, the length should be equal to the number of objectives except accuracy. For Static controller, the length should be the number of all objectives including accuracy and the last weight is for accuracy.\n",
    "- early_stop: early_stop epochs. In multi-objective settings, we could not set early stopping by minitoring one objective, so we set to -1 to disable early stopping.\n",
    "\n",
    "\n",
    "Note that you could obtain diverse solutions by setting various `morec_expect_loss` and `morec_objective_weights`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] MF-Morec-Finetune: config={'gpu_id': 0, 'use_gpu': True, 'seed': 2022, 'state': 'INFO', 'verbose': 2, 'saved': True, 'use_tensorboard': 1, 'use_wandb': False, 'init_method': 'normal', 'init_std': 0.02, 'init_mean': 0.0, 'scheduler': None, 'scheduler_factor': 0.1, 'time_seq': 0, 'seq_last': False, 'has_user_emb': True, 'has_user_bias': 0, 'has_item_bias': 0, 'use_features': False, 'use_text_emb': False, 'use_position_emb': True, 'load_pretrained_model': False, 'embedding_size': 64, 'hidden_size': 128, 'inner_size': 128, 'dropout_prob': 0.0, 'epochs': 30, 'batch_size': 512, 'learning_rate': 0.001, 'optimizer': 'adam', 'eval_step': 1, 'early_stop': -1, 'clip_grad_norm': None, 'weight_decay': 1e-06, 'num_workers': 4, 'persistent_workers': False, 'pin_memory': False, 'shuffle_train': False, 'use_pre_item_emb': 0, 'loss_type': 'bpr', 'ccl_w': 150, 'ccl_m': 0.4, 'distance_type': 'dot', 'metrics': \"['hit@10', 'ndcg@10', 'rhit@10', 'rndcg@10', 'pop-kl@10', 'least-misery']\", 'key_metric': 'ndcg@10', 'test_protocol': 'one_vs_all', 'valid_protocol': 'one_vs_all', 'test_batch_size': 100, 'model': 'MF', 'dataloader': 'BaseDataset', 'max_seq_len': 20, 'history_mask_mode': 'autoagressive', 'tau': 1.0, 'enable_morec': 1, 'morec_objectives': ['fairness', 'alignment', 'revenue'], 'morec_objective_controller': 'PID', 'morec_ngroup': 40, 'morec_alpha': 0.1, 'morec_lambda': 0.2, 'morec_expect_loss': 0.2, 'morec_beta_min': 0.1, 'morec_beta_max': 1.5, 'morec_K_p': 0.05, 'morec_K_i': 0.001, 'morec_objective_weights': '[0.2,0.2,0.6]', 'group_size': -1, 'n_items': 44848, 'n_neg_test_from_sampling': 0, 'n_neg_train_from_sampling': 0, 'n_neg_valid_from_sampling': 0, 'n_users': 124917, 'test_file_format': 'user-item', 'train_file_format': 'user-item', 'user_history_file_format': 'user-item_seq', 'valid_file_format': 'user-item', 'config_dir': '/anaconda/envs/new-unirec/lib/python3.9/site-packages/unirec/config', 'exp_name': 'MF-Morec-Finetune', 'checkpoint_dir': 'morec_finetune_2023-10-28_11-35-22', 'dataset': 'Electronics', 'dataset_path': '/home/v-huangxu/.unirec/dataset/binary/Electronics', 'output_path': '/home/v-huangxu/.unirec/output/Electronics/MF', 'user_pre_item_emb': 0, 'valid_batch_size': 1024, 'n_sample_neg_train': 10, 'neg_by_pop_alpha': 1.0, 'grad_clip_value': -1, 'user_history_filename': 'user_history', 'num_workers_test': 0, 'item_meta_morec_filename': 'item_meta_morec.csv', 'align_dist_filename': None, 'model_file': '/home/v-huangxu/.unirec/output/Electronics/MF/morec_pretrain_2023-10-28_11-35-22/MF-MoRec-Pretrain.pth', 'cmd_args': {'config_dir': '/anaconda/envs/new-unirec/lib/python3.9/site-packages/unirec/config', 'exp_name': 'MF-Morec-Finetune', 'checkpoint_dir': 'morec_finetune_2023-10-28_11-35-22', 'model': 'MF', 'dataloader': 'BaseDataset', 'dataset': 'Electronics', 'dataset_path': '/home/v-huangxu/.unirec/dataset/binary/Electronics', 'output_path': '/home/v-huangxu/.unirec/output/Electronics/MF', 'learning_rate': 0.001, 'scheduler': None, 'dropout_prob': 0.0, 'embedding_size': 64, 'user_pre_item_emb': 0, 'loss_type': 'bpr', 'max_seq_len': 20, 'has_user_bias': 0, 'has_item_bias': 0, 'epochs': 30, 'early_stop': -1, 'batch_size': 512, 'valid_batch_size': 1024, 'n_sample_neg_train': 10, 'neg_by_pop_alpha': 1.0, 'valid_protocol': 'one_vs_all', 'test_protocol': 'one_vs_all', 'grad_clip_value': -1, 'weight_decay': 1e-06, 'history_mask_mode': 'autoagressive', 'user_history_filename': 'user_history', 'metrics': \"['hit@10', 'ndcg@10', 'rhit@10', 'rndcg@10', 'pop-kl@10', 'least-misery']\", 'key_metric': 'ndcg@10', 'num_workers': 4, 'num_workers_test': 0, 'verbose': 2, 'item_meta_morec_filename': 'item_meta_morec.csv', 'align_dist_filename': None, 'use_tensorboard': 1, 'seed': 2022, 'enable_morec': 1, 'model_file': '/home/v-huangxu/.unirec/output/Electronics/MF/morec_pretrain_2023-10-28_11-35-22/MF-MoRec-Pretrain.pth', 'morec_objectives': ['fairness', 'alignment', 'revenue'], 'morec_ngroup': 40, 'morec_alpha': 0.1, 'morec_lambda': 0.2, 'morec_expect_loss': 0.2, 'morec_beta_min': 0.1, 'morec_beta_max': 1.5, 'morec_K_p': 0.05, 'morec_K_i': 0.001, 'morec_objective_controller': 'PID', 'morec_objective_weights': '[0.2,0.2,0.6]', 'logger_time_str': '2023-10-28_140001', 'logger_rand': 23}, 'device': device(type='cuda'), 'task': 'train', 'logger_time_str': '2023-10-28_140001', 'logger_rand': 23}\n",
      "[INFO] MF-Morec-Finetune: Loading user history from user_history ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load configuration files from /anaconda/envs/new-unirec/lib/python3.9/site-packages/unirec/config\n",
      "Writing logs to /home/v-huangxu/.unirec/output/Electronics/MF/MF-Morec-Finetune.2023-10-28_140001.23.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] MF-Morec-Finetune: Done. 124917 of users have history.\n",
      "[INFO] MF-Morec-Finetune: Loading model from checkpoint: /home/v-huangxu/.unirec/output/Electronics/MF/morec_pretrain_2023-10-28_11-35-22/MF-MoRec-Pretrain.pth ...\n",
      "[INFO] MF-Morec-Finetune: Constructing dataset of task type: train\n",
      "[DEBUG] MF-Morec-Finetune: loading train at 28/10/2023 14:00:03\n",
      "[DEBUG] MF-Morec-Finetune: Finished loading train at 28/10/2023 14:00:03\n",
      "[INFO] MF-Morec-Finetune: Finished initializing <class 'unirec.data.dataset.basedataset.BaseDataset'>\n",
      "[INFO] MF-Morec-Finetune: Constructing dataset of task type: train\n",
      "[DEBUG] MF-Morec-Finetune: loading valid at 28/10/2023 14:00:03\n",
      "[DEBUG] MF-Morec-Finetune: Finished loading valid at 28/10/2023 14:00:03\n",
      "[INFO] MF-Morec-Finetune: Finished initializing <class 'unirec.data.dataset.basedataset.BaseDataset'>\n",
      "[INFO] MF-Morec-Finetune: Constructing dataset of task type: valid\n",
      "[DEBUG] MF-Morec-Finetune: loading valid at 28/10/2023 14:00:03\n",
      "[DEBUG] MF-Morec-Finetune: Finished loading valid at 28/10/2023 14:00:03\n",
      "[INFO] MF-Morec-Finetune: Finished initializing <class 'unirec.data.dataset.basedataset.BaseDataset'>\n",
      "[INFO] MF-Morec-Finetune: MF(\n",
      "  (scorer_layers): InnerProductScorer()\n",
      "  (user_embedding): Embedding(124917, 64, padding_idx=0)\n",
      "  (item_embedding): Embedding(44848, 64, padding_idx=0)\n",
      ")\n",
      "Trainable parameter number: 10864960\n",
      "All trainable parameters:\n",
      "user_embedding.weight : torch.Size([124917, 64])\n",
      "item_embedding.weight : torch.Size([44848, 64])\n",
      "[INFO] MF-Morec-Finetune: tensorboard log file saved in /home/v-huangxu/.unirec/output/Electronics/MF/morec_finetune_2023-10-28_11-35-22\n",
      "[DEBUG] MF-Morec-Finetune: >> Valid before training...\n",
      "[INFO] MF-Morec-Finetune: one_vs_all\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "static weight: [0.2, 0.2, 0.6].\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluate: 100%|██████████| 122/122 [01:22<00:00,  1.48it/s]\n",
      "[INFO] MF-Morec-Finetune: epoch 0 evaluating [time: 82.61s, ndcg@10: 0.015853]\n",
      "[INFO] MF-Morec-Finetune: complete scores on valid set: \n",
      "hit@10:0.028226968522847354 min-hit@10:0.018010963194988253 min-ndcg@10:0.009588962031888805 min-rhit@10:0.5446411783140082 min-rndcg@10:0.27312923683920476 ndcg@10:0.015853226085775617 pop-kl@10:1.4318585098193302 rhit@10:2.5707144000768514 rndcg@10:1.4293092277158326\n",
      "[INFO] MF-Morec-Finetune: Saving best model at epoch 0 to /home/v-huangxu/.unirec/output/Electronics/MF/morec_finetune_2023-10-28_11-35-22/MF-Morec-Finetune.pth\n",
      "[INFO] MF-Morec-Finetune: \n",
      ">> epoch 1\n",
      "Train: 100%|██████████| 1608/1608 [00:56<00:00, 28.56it/s]\n",
      "[INFO] MF-Morec-Finetune: epoch 1 training [time: 56.31s, train loss: 198.4705]\n",
      "[INFO] MF-Morec-Finetune: one_vs_all\n",
      "Evaluate: 100%|██████████| 122/122 [01:20<00:00,  1.52it/s]\n",
      "[INFO] MF-Morec-Finetune: epoch 1 evaluating [time: 80.21s, ndcg@10: 0.014038]\n",
      "[INFO] MF-Morec-Finetune: complete scores on valid set: \n",
      "hit@10:0.02482468218642928 min-hit@10:0.00797968806673921 min-ndcg@10:0.0036278382594145775 min-rhit@10:0.4365215814290896 min-rndcg@10:0.18963825961500252 ndcg@10:0.014037656533280847 pop-kl@10:0.33685819153238716 rhit@10:3.7410295718722986 rndcg@10:2.1165091022809706\n",
      "[INFO] MF-Morec-Finetune: Saving best model at epoch 1 to /home/v-huangxu/.unirec/output/Electronics/MF/morec_finetune_2023-10-28_11-35-22/MF-Morec-Finetune.pth\n",
      "[INFO] MF-Morec-Finetune: \n",
      ">> epoch 2\n",
      "Train: 100%|██████████| 1608/1608 [00:56<00:00, 28.65it/s]\n",
      "[INFO] MF-Morec-Finetune: epoch 2 training [time: 56.14s, train loss: 279.7193]\n",
      "[INFO] MF-Morec-Finetune: one_vs_all\n",
      "Evaluate: 100%|██████████| 122/122 [01:19<00:00,  1.54it/s]\n",
      "[INFO] MF-Morec-Finetune: epoch 2 evaluating [time: 79.05s, ndcg@10: 0.014985]\n",
      "[INFO] MF-Morec-Finetune: complete scores on valid set: \n",
      "hit@10:0.026393736590989144 min-hit@10:0.010155966630395358 min-ndcg@10:0.004735090151260112 min-rhit@10:0.42717446499818645 min-rndcg@10:0.17157510037778453 ndcg@10:0.014984860587716064 pop-kl@10:0.21698053099470663 rhit@10:3.6349151429760798 rndcg@10:2.0516326565030836\n",
      "[INFO] MF-Morec-Finetune: Saving best model at epoch 2 to /home/v-huangxu/.unirec/output/Electronics/MF/morec_finetune_2023-10-28_11-35-22/MF-Morec-Finetune.pth\n",
      "[INFO] MF-Morec-Finetune: \n",
      ">> epoch 3\n",
      "Train: 100%|██████████| 1608/1608 [00:56<00:00, 28.56it/s]\n",
      "[INFO] MF-Morec-Finetune: epoch 3 training [time: 56.30s, train loss: 296.2911]\n",
      "[INFO] MF-Morec-Finetune: one_vs_all\n",
      "Evaluate: 100%|██████████| 122/122 [01:19<00:00,  1.53it/s]\n",
      "[INFO] MF-Morec-Finetune: epoch 3 evaluating [time: 79.74s, ndcg@10: 0.015136]\n",
      "[INFO] MF-Morec-Finetune: complete scores on valid set: \n",
      "hit@10:0.026826027090204617 min-hit@10:0.011606819006166122 min-ndcg@10:0.0052300083490927855 min-rhit@10:0.5126840768951759 min-rndcg@10:0.20506735767842896 ndcg@10:0.015136304388881397 pop-kl@10:0.1811128483720397 rhit@10:3.398083672227737 rndcg@10:1.9186839954689283\n",
      "[INFO] MF-Morec-Finetune: Saving best model at epoch 3 to /home/v-huangxu/.unirec/output/Electronics/MF/morec_finetune_2023-10-28_11-35-22/MF-Morec-Finetune.pth\n",
      "[INFO] MF-Morec-Finetune: \n",
      ">> epoch 4\n",
      "Train: 100%|██████████| 1608/1608 [00:59<00:00, 27.12it/s]\n",
      "[INFO] MF-Morec-Finetune: epoch 4 training [time: 59.29s, train loss: 285.3903]\n",
      "[INFO] MF-Morec-Finetune: one_vs_all\n",
      "Evaluate: 100%|██████████| 122/122 [01:21<00:00,  1.50it/s]\n",
      "[INFO] MF-Morec-Finetune: epoch 4 evaluating [time: 81.59s, ndcg@10: 0.015377]\n",
      "[INFO] MF-Morec-Finetune: complete scores on valid set: \n",
      "hit@10:0.0273063498671107 min-hit@10:0.013057671381936888 min-ndcg@10:0.006816080072276732 min-rhit@10:0.43358360536815377 min-rndcg@10:0.23869660371463144 ndcg@10:0.015376984875250866 pop-kl@10:0.23039106826372746 rhit@10:3.3828513561113067 rndcg@10:1.8963313742417724\n",
      "[INFO] MF-Morec-Finetune: Saving best model at epoch 4 to /home/v-huangxu/.unirec/output/Electronics/MF/morec_finetune_2023-10-28_11-35-22/MF-Morec-Finetune.pth\n",
      "[INFO] MF-Morec-Finetune: \n",
      ">> epoch 5\n",
      "Train: 100%|██████████| 1608/1608 [00:56<00:00, 28.36it/s]\n",
      "[INFO] MF-Morec-Finetune: epoch 5 training [time: 56.70s, train loss: 268.1945]\n",
      "[INFO] MF-Morec-Finetune: one_vs_all\n",
      "Evaluate: 100%|██████████| 122/122 [01:19<00:00,  1.53it/s]\n",
      "[INFO] MF-Morec-Finetune: epoch 5 evaluating [time: 79.69s, ndcg@10: 0.015410]\n",
      "[INFO] MF-Morec-Finetune: complete scores on valid set: \n",
      "hit@10:0.027466457459412726 min-hit@10:0.014871236851650345 min-ndcg@10:0.007261233638927043 min-rhit@10:0.4984983677910773 min-rndcg@10:0.23239155310680226 ndcg@10:0.015409669438439183 pop-kl@10:0.2586823566650117 rhit@10:3.3034981907842074 rndcg@10:1.8683041377979612\n",
      "[INFO] MF-Morec-Finetune: Saving best model at epoch 5 to /home/v-huangxu/.unirec/output/Electronics/MF/morec_finetune_2023-10-28_11-35-22/MF-Morec-Finetune.pth\n",
      "[INFO] MF-Morec-Finetune: \n",
      ">> epoch 6\n",
      "Train: 100%|██████████| 1608/1608 [00:56<00:00, 28.46it/s]\n",
      "[INFO] MF-Morec-Finetune: epoch 6 training [time: 56.51s, train loss: 253.0301]\n",
      "[INFO] MF-Morec-Finetune: one_vs_all\n",
      "Evaluate: 100%|██████████| 122/122 [01:20<00:00,  1.52it/s]\n",
      "[INFO] MF-Morec-Finetune: epoch 6 evaluating [time: 80.06s, ndcg@10: 0.015590]\n",
      "[INFO] MF-Morec-Finetune: complete scores on valid set: \n",
      "hit@10:0.02778667264401678 min-hit@10:0.016684802321363802 min-ndcg@10:0.00842619496985019 min-rhit@10:0.4980848748639826 min-rndcg@10:0.23731909908820548 ndcg@10:0.015590411541392263 pop-kl@10:0.2898647564615594 rhit@10:3.2929133177495276 rndcg@10:1.855674345575114\n",
      "[INFO] MF-Morec-Finetune: Saving best model at epoch 6 to /home/v-huangxu/.unirec/output/Electronics/MF/morec_finetune_2023-10-28_11-35-22/MF-Morec-Finetune.pth\n",
      "[INFO] MF-Morec-Finetune: \n",
      ">> epoch 7\n",
      "Train: 100%|██████████| 1608/1608 [00:56<00:00, 28.39it/s]\n",
      "[INFO] MF-Morec-Finetune: epoch 7 training [time: 56.64s, train loss: 240.6037]\n",
      "[INFO] MF-Morec-Finetune: one_vs_all\n",
      "Evaluate: 100%|██████████| 122/122 [01:18<00:00,  1.55it/s]\n",
      "[INFO] MF-Morec-Finetune: epoch 7 evaluating [time: 78.89s, ndcg@10: 0.015704]\n",
      "[INFO] MF-Morec-Finetune: complete scores on valid set: \n",
      "hit@10:0.027930769477088602 min-hit@10:0.015233949945593036 min-ndcg@10:0.007535663581667236 min-rhit@10:0.42527384838592674 min-rndcg@10:0.19363788685662586 ndcg@10:0.015704168128942632 pop-kl@10:0.3038447495716581 rhit@10:3.3188999007332933 rndcg@10:1.9047819237301655\n",
      "[INFO] MF-Morec-Finetune: Saving best model at epoch 7 to /home/v-huangxu/.unirec/output/Electronics/MF/morec_finetune_2023-10-28_11-35-22/MF-Morec-Finetune.pth\n",
      "[INFO] MF-Morec-Finetune: \n",
      ">> epoch 8\n",
      "Train: 100%|██████████| 1608/1608 [00:56<00:00, 28.65it/s]\n",
      "[INFO] MF-Morec-Finetune: epoch 8 training [time: 56.13s, train loss: 232.0542]\n",
      "[INFO] MF-Morec-Finetune: one_vs_all\n",
      "Evaluate: 100%|██████████| 122/122 [01:19<00:00,  1.54it/s]\n",
      "[INFO] MF-Morec-Finetune: epoch 8 evaluating [time: 79.26s, ndcg@10: 0.015666]\n",
      "[INFO] MF-Morec-Finetune: complete scores on valid set: \n",
      "hit@10:0.027810688782862084 min-hit@10:0.01632208922742111 min-ndcg@10:0.007880847226909495 min-rhit@10:0.5496082698585418 min-rndcg@10:0.24609423422485752 ndcg@10:0.0156656247324709 pop-kl@10:0.31760483380344284 rhit@10:3.3006024848698323 rndcg@10:1.8831737037464633\n",
      "[INFO] MF-Morec-Finetune: Saving best model at epoch 8 to /home/v-huangxu/.unirec/output/Electronics/MF/morec_finetune_2023-10-28_11-35-22/MF-Morec-Finetune.pth\n",
      "[INFO] MF-Morec-Finetune: \n",
      ">> epoch 9\n",
      "Train: 100%|██████████| 1608/1608 [00:56<00:00, 28.62it/s]\n",
      "[INFO] MF-Morec-Finetune: epoch 9 training [time: 56.19s, train loss: 226.4667]\n",
      "[INFO] MF-Morec-Finetune: one_vs_all\n",
      "Evaluate: 100%|██████████| 122/122 [01:18<00:00,  1.55it/s]\n",
      "[INFO] MF-Morec-Finetune: epoch 9 evaluating [time: 78.90s, ndcg@10: 0.015833]\n",
      "[INFO] MF-Morec-Finetune: complete scores on valid set: \n",
      "hit@10:0.02820295238400205 min-hit@10:0.01595937613347842 min-ndcg@10:0.0069301821447155 min-rhit@10:0.5479833151976786 min-rndcg@10:0.22178868751019476 ndcg@10:0.015833318178368284 pop-kl@10:0.31866682294627674 rhit@10:3.393127541708028 rndcg@10:1.952926596959668\n",
      "[INFO] MF-Morec-Finetune: Saving best model at epoch 9 to /home/v-huangxu/.unirec/output/Electronics/MF/morec_finetune_2023-10-28_11-35-22/MF-Morec-Finetune.pth\n",
      "[INFO] MF-Morec-Finetune: \n",
      ">> epoch 10\n",
      "Train: 100%|██████████| 1608/1608 [00:56<00:00, 28.30it/s]\n",
      "[INFO] MF-Morec-Finetune: epoch 10 training [time: 56.82s, train loss: 223.7140]\n",
      "[INFO] MF-Morec-Finetune: one_vs_all\n",
      "Evaluate: 100%|██████████| 122/122 [01:19<00:00,  1.54it/s]\n",
      "[INFO] MF-Morec-Finetune: epoch 10 evaluating [time: 79.43s, ndcg@10: 0.015669]\n",
      "[INFO] MF-Morec-Finetune: complete scores on valid set: \n",
      "hit@10:0.0278987479586282 min-hit@10:0.015233949945593036 min-ndcg@10:0.007556960910908916 min-rhit@10:0.5359231048240841 min-rndcg@10:0.22352079450602674 ndcg@10:0.01566863074394135 pop-kl@10:0.3188653420549532 rhit@10:3.419255099426816 rndcg@10:1.941344231826134\n",
      "[INFO] MF-Morec-Finetune: Saving best model at epoch 10 to /home/v-huangxu/.unirec/output/Electronics/MF/morec_finetune_2023-10-28_11-35-22/MF-Morec-Finetune.pth\n",
      "[INFO] MF-Morec-Finetune: \n",
      ">> epoch 11\n",
      "Train: 100%|██████████| 1608/1608 [00:56<00:00, 28.25it/s]\n",
      "[INFO] MF-Morec-Finetune: epoch 11 training [time: 56.92s, train loss: 222.3318]\n",
      "[INFO] MF-Morec-Finetune: one_vs_all\n",
      "Evaluate: 100%|██████████| 122/122 [01:18<00:00,  1.55it/s]\n",
      "[INFO] MF-Morec-Finetune: epoch 11 evaluating [time: 78.80s, ndcg@10: 0.015871]\n",
      "[INFO] MF-Morec-Finetune: complete scores on valid set: \n",
      "hit@10:0.02826699542092286 min-hit@10:0.015233949945593036 min-ndcg@10:0.007433701252046386 min-rhit@10:0.6686724700761697 min-rndcg@10:0.27781221604139084 ndcg@10:0.015870766328816375 pop-kl@10:0.32854627030064576 rhit@10:3.4636250760511063 rndcg@10:1.9922828480512327\n",
      "[INFO] MF-Morec-Finetune: Saving best model at epoch 11 to /home/v-huangxu/.unirec/output/Electronics/MF/morec_finetune_2023-10-28_11-35-22/MF-Morec-Finetune.pth\n",
      "[INFO] MF-Morec-Finetune: \n",
      ">> epoch 12\n",
      "Train: 100%|██████████| 1608/1608 [00:56<00:00, 28.61it/s]\n",
      "[INFO] MF-Morec-Finetune: epoch 12 training [time: 56.22s, train loss: 222.1721]\n",
      "[INFO] MF-Morec-Finetune: one_vs_all\n",
      "Evaluate: 100%|██████████| 122/122 [01:19<00:00,  1.53it/s]\n",
      "[INFO] MF-Morec-Finetune: epoch 12 evaluating [time: 79.96s, ndcg@10: 0.015831]\n",
      "[INFO] MF-Morec-Finetune: complete scores on valid set: \n",
      "hit@10:0.028114893208235934 min-hit@10:0.015233949945593036 min-ndcg@10:0.007422983058098209 min-rhit@10:0.5335545883206383 min-rndcg@10:0.2471955284318293 ndcg@10:0.01583070688021408 pop-kl@10:0.3314847600369328 rhit@10:3.4623910467834382 rndcg@10:1.985619212735809\n",
      "[INFO] MF-Morec-Finetune: Saving best model at epoch 12 to /home/v-huangxu/.unirec/output/Electronics/MF/morec_finetune_2023-10-28_11-35-22/MF-Morec-Finetune.pth\n",
      "[INFO] MF-Morec-Finetune: \n",
      ">> epoch 13\n",
      "Train: 100%|██████████| 1608/1608 [00:56<00:00, 28.45it/s]\n",
      "[INFO] MF-Morec-Finetune: epoch 13 training [time: 56.52s, train loss: 222.6367]\n",
      "[INFO] MF-Morec-Finetune: one_vs_all\n",
      "Evaluate: 100%|██████████| 122/122 [01:20<00:00,  1.52it/s]\n",
      "[INFO] MF-Morec-Finetune: epoch 13 evaluating [time: 80.28s, ndcg@10: 0.015857]\n",
      "[INFO] MF-Morec-Finetune: complete scores on valid set: \n",
      "hit@10:0.02826699542092286 min-hit@10:0.014145810663764961 min-ndcg@10:0.0069439541543202445 min-rhit@10:0.49641639463184617 min-rndcg@10:0.2396458441317699 ndcg@10:0.015857096889667084 pop-kl@10:0.33157736188648157 rhit@10:3.4656838195267223 rndcg@10:1.986524646731767\n",
      "[INFO] MF-Morec-Finetune: Saving best model at epoch 13 to /home/v-huangxu/.unirec/output/Electronics/MF/morec_finetune_2023-10-28_11-35-22/MF-Morec-Finetune.pth\n",
      "[INFO] MF-Morec-Finetune: \n",
      ">> epoch 14\n",
      "Train: 100%|██████████| 1608/1608 [00:56<00:00, 28.54it/s]\n",
      "[INFO] MF-Morec-Finetune: epoch 14 training [time: 56.34s, train loss: 222.9764]\n",
      "[INFO] MF-Morec-Finetune: one_vs_all\n",
      "Evaluate: 100%|██████████| 122/122 [01:19<00:00,  1.53it/s]\n",
      "[INFO] MF-Morec-Finetune: epoch 14 evaluating [time: 79.75s, ndcg@10: 0.015754]\n",
      "[INFO] MF-Morec-Finetune: complete scores on valid set: \n",
      "hit@10:0.027954785615933907 min-hit@10:0.014508523757707652 min-ndcg@10:0.0073162452015138124 min-rhit@10:0.5091439970982953 min-rndcg@10:0.260821318619538 ndcg@10:0.0157538185256068 pop-kl@10:0.34102499769065364 rhit@10:3.4308271958756285 rndcg@10:1.9640638832467676\n",
      "[INFO] MF-Morec-Finetune: Saving best model at epoch 14 to /home/v-huangxu/.unirec/output/Electronics/MF/morec_finetune_2023-10-28_11-35-22/MF-Morec-Finetune.pth\n",
      "[INFO] MF-Morec-Finetune: \n",
      ">> epoch 15\n",
      "Train: 100%|██████████| 1608/1608 [00:56<00:00, 28.50it/s]\n",
      "[INFO] MF-Morec-Finetune: epoch 15 training [time: 56.43s, train loss: 224.0289]\n",
      "[INFO] MF-Morec-Finetune: one_vs_all\n",
      "Evaluate: 100%|██████████| 122/122 [01:19<00:00,  1.53it/s]\n",
      "[INFO] MF-Morec-Finetune: epoch 15 evaluating [time: 79.89s, ndcg@10: 0.015983]\n",
      "[INFO] MF-Morec-Finetune: complete scores on valid set: \n",
      "hit@10:0.028299016939383265 min-hit@10:0.014871236851650345 min-ndcg@10:0.007186667936516551 min-rhit@10:0.6215820536926773 min-rndcg@10:0.29575728966077314 ndcg@10:0.015982643889056326 pop-kl@10:0.3300114050255375 rhit@10:3.5353095680297164 rndcg@10:2.0318685267656655\n",
      "[INFO] MF-Morec-Finetune: Saving best model at epoch 15 to /home/v-huangxu/.unirec/output/Electronics/MF/morec_finetune_2023-10-28_11-35-22/MF-Morec-Finetune.pth\n",
      "[INFO] MF-Morec-Finetune: \n",
      ">> epoch 16\n",
      "Train: 100%|██████████| 1608/1608 [00:57<00:00, 28.09it/s]\n",
      "[INFO] MF-Morec-Finetune: epoch 16 training [time: 57.25s, train loss: 224.6618]\n",
      "[INFO] MF-Morec-Finetune: one_vs_all\n",
      "Evaluate: 100%|██████████| 122/122 [01:19<00:00,  1.53it/s]\n",
      "[INFO] MF-Morec-Finetune: epoch 16 evaluating [time: 79.95s, ndcg@10: 0.015892]\n",
      "[INFO] MF-Morec-Finetune: complete scores on valid set: \n",
      "hit@10:0.028363059976304075 min-hit@10:0.014871236851650345 min-ndcg@10:0.007034688231737302 min-rhit@10:0.5114689880304678 min-rndcg@10:0.2430843604491403 ndcg@10:0.015892018671198445 pop-kl@10:0.32493041620149554 rhit@10:3.4555458067821574 rndcg@10:1.9527205807703407\n",
      "[INFO] MF-Morec-Finetune: Saving best model at epoch 16 to /home/v-huangxu/.unirec/output/Electronics/MF/morec_finetune_2023-10-28_11-35-22/MF-Morec-Finetune.pth\n",
      "[INFO] MF-Morec-Finetune: \n",
      ">> epoch 17\n",
      "Train: 100%|██████████| 1608/1608 [00:59<00:00, 27.16it/s]\n",
      "[INFO] MF-Morec-Finetune: epoch 17 training [time: 59.21s, train loss: 224.6995]\n",
      "[INFO] MF-Morec-Finetune: one_vs_all\n",
      "Evaluate: 100%|██████████| 122/122 [01:23<00:00,  1.46it/s]\n",
      "[INFO] MF-Morec-Finetune: epoch 17 evaluating [time: 83.54s, ndcg@10: 0.015850]\n",
      "[INFO] MF-Morec-Finetune: complete scores on valid set: \n",
      "hit@10:0.028371065355919177 min-hit@10:0.014508523757707652 min-ndcg@10:0.007040550600170063 min-rhit@10:0.6233405875952122 min-rndcg@10:0.26200705266211183 ndcg@10:0.015849503867320897 pop-kl@10:0.33375818832379567 rhit@10:3.5122905792692687 rndcg@10:1.9918910161262635\n",
      "[INFO] MF-Morec-Finetune: Saving best model at epoch 17 to /home/v-huangxu/.unirec/output/Electronics/MF/morec_finetune_2023-10-28_11-35-22/MF-Morec-Finetune.pth\n",
      "[INFO] MF-Morec-Finetune: \n",
      ">> epoch 18\n",
      "Train: 100%|██████████| 1608/1608 [00:58<00:00, 27.60it/s]\n",
      "[INFO] MF-Morec-Finetune: epoch 18 training [time: 58.27s, train loss: 225.0630]\n",
      "[INFO] MF-Morec-Finetune: one_vs_all\n",
      "Evaluate: 100%|██████████| 122/122 [01:20<00:00,  1.51it/s]\n",
      "[INFO] MF-Morec-Finetune: epoch 18 evaluating [time: 80.66s, ndcg@10: 0.015847]\n",
      "[INFO] MF-Morec-Finetune: complete scores on valid set: \n",
      "hit@10:0.028299016939383265 min-hit@10:0.01632208922742111 min-ndcg@10:0.00790490947181816 min-rhit@10:0.6384769664681919 min-rndcg@10:0.30869121767841784 ndcg@10:0.015846520582311792 pop-kl@10:0.3444185572468261 rhit@10:3.4756787761375647 rndcg@10:1.9922358941833092\n",
      "[INFO] MF-Morec-Finetune: Saving best model at epoch 18 to /home/v-huangxu/.unirec/output/Electronics/MF/morec_finetune_2023-10-28_11-35-22/MF-Morec-Finetune.pth\n",
      "[INFO] MF-Morec-Finetune: \n",
      ">> epoch 19\n",
      "Train: 100%|██████████| 1608/1608 [00:56<00:00, 28.54it/s]\n",
      "[INFO] MF-Morec-Finetune: epoch 19 training [time: 56.35s, train loss: 225.1295]\n",
      "[INFO] MF-Morec-Finetune: one_vs_all\n",
      "Evaluate: 100%|██████████| 122/122 [01:19<00:00,  1.53it/s]\n",
      "[INFO] MF-Morec-Finetune: epoch 19 evaluating [time: 79.68s, ndcg@10: 0.015753]\n",
      "[INFO] MF-Morec-Finetune: complete scores on valid set: \n",
      "hit@10:0.02796279099554901 min-hit@10:0.01378309756982227 min-ndcg@10:0.007095579374260328 min-rhit@10:0.6267500906782736 min-rndcg@10:0.27569327264594157 ndcg@10:0.015753374919920467 pop-kl@10:0.3285114206867692 rhit@10:3.4630071407986165 rndcg@10:1.9999391437238778\n",
      "[INFO] MF-Morec-Finetune: Saving best model at epoch 19 to /home/v-huangxu/.unirec/output/Electronics/MF/morec_finetune_2023-10-28_11-35-22/MF-Morec-Finetune.pth\n",
      "[INFO] MF-Morec-Finetune: \n",
      ">> epoch 20\n",
      "Train: 100%|██████████| 1608/1608 [00:56<00:00, 28.46it/s]\n",
      "[INFO] MF-Morec-Finetune: epoch 20 training [time: 56.50s, train loss: 224.9188]\n",
      "[INFO] MF-Morec-Finetune: one_vs_all\n",
      "Evaluate: 100%|██████████| 122/122 [01:20<00:00,  1.52it/s]\n",
      "[INFO] MF-Morec-Finetune: epoch 20 evaluating [time: 80.20s, ndcg@10: 0.015853]\n",
      "[INFO] MF-Morec-Finetune: complete scores on valid set: \n",
      "hit@10:0.028066860930545327 min-hit@10:0.01342038447587958 min-ndcg@10:0.006773800357200351 min-rhit@10:0.5527022125498731 min-rndcg@10:0.2630151395969696 ndcg@10:0.015852541772925798 pop-kl@10:0.34033070064906756 rhit@10:3.4681629254859265 rndcg@10:1.9909926309660357\n",
      "[INFO] MF-Morec-Finetune: Saving best model at epoch 20 to /home/v-huangxu/.unirec/output/Electronics/MF/morec_finetune_2023-10-28_11-35-22/MF-Morec-Finetune.pth\n",
      "[INFO] MF-Morec-Finetune: \n",
      ">> epoch 21\n",
      "Train: 100%|██████████| 1608/1608 [00:56<00:00, 28.54it/s]\n",
      "[INFO] MF-Morec-Finetune: epoch 21 training [time: 56.34s, train loss: 225.2269]\n",
      "[INFO] MF-Morec-Finetune: one_vs_all\n",
      "Evaluate: 100%|██████████| 122/122 [01:19<00:00,  1.53it/s]\n",
      "[INFO] MF-Morec-Finetune: epoch 21 evaluating [time: 79.86s, ndcg@10: 0.015848]\n",
      "[INFO] MF-Morec-Finetune: complete scores on valid set: \n",
      "hit@10:0.028291011559768164 min-hit@10:0.01342038447587958 min-ndcg@10:0.0067937870978291345 min-rhit@10:0.43065651070003635 min-rndcg@10:0.23059906809444625 ndcg@10:0.01584770575711528 pop-kl@10:0.3360104934355098 rhit@10:3.4541977008549756 rndcg@10:1.9679594284536288\n",
      "[INFO] MF-Morec-Finetune: Saving best model at epoch 21 to /home/v-huangxu/.unirec/output/Electronics/MF/morec_finetune_2023-10-28_11-35-22/MF-Morec-Finetune.pth\n",
      "[INFO] MF-Morec-Finetune: \n",
      ">> epoch 22\n",
      "Train: 100%|██████████| 1608/1608 [00:56<00:00, 28.42it/s]\n",
      "[INFO] MF-Morec-Finetune: epoch 22 training [time: 56.58s, train loss: 225.0681]\n",
      "[INFO] MF-Morec-Finetune: one_vs_all\n",
      "Evaluate: 100%|██████████| 122/122 [01:19<00:00,  1.54it/s]\n",
      "[INFO] MF-Morec-Finetune: epoch 22 evaluating [time: 79.09s, ndcg@10: 0.016043]\n",
      "[INFO] MF-Morec-Finetune: complete scores on valid set: \n",
      "hit@10:0.02842710301322489 min-hit@10:0.014145810663764961 min-ndcg@10:0.006929995300303152 min-rhit@10:0.609227421109902 min-rndcg@10:0.25573169119081307 ndcg@10:0.01604250127192806 pop-kl@10:0.34938861691221923 rhit@10:3.4823709532806046 rndcg@10:2.0042650073174144\n",
      "[INFO] MF-Morec-Finetune: Saving best model at epoch 22 to /home/v-huangxu/.unirec/output/Electronics/MF/morec_finetune_2023-10-28_11-35-22/MF-Morec-Finetune.pth\n",
      "[INFO] MF-Morec-Finetune: \n",
      ">> epoch 23\n",
      "Train: 100%|██████████| 1608/1608 [00:56<00:00, 28.48it/s]\n",
      "[INFO] MF-Morec-Finetune: epoch 23 training [time: 56.46s, train loss: 224.7042]\n",
      "[INFO] MF-Morec-Finetune: one_vs_all\n",
      "Evaluate: 100%|██████████| 122/122 [01:18<00:00,  1.55it/s]\n",
      "[INFO] MF-Morec-Finetune: epoch 23 evaluating [time: 78.75s, ndcg@10: 0.015865]\n",
      "[INFO] MF-Morec-Finetune: complete scores on valid set: \n",
      "hit@10:0.028299016939383265 min-hit@10:0.013057671381936888 min-ndcg@10:0.00740820155844267 min-rhit@10:0.41153790351831704 min-rndcg@10:0.23659416544995365 ndcg@10:0.0158653632550109 pop-kl@10:0.34765349892934017 rhit@10:3.47175061641423 rndcg@10:2.0010665904194727\n",
      "[INFO] MF-Morec-Finetune: Saving best model at epoch 23 to /home/v-huangxu/.unirec/output/Electronics/MF/morec_finetune_2023-10-28_11-35-22/MF-Morec-Finetune.pth\n",
      "[INFO] MF-Morec-Finetune: \n",
      ">> epoch 24\n",
      "Train: 100%|██████████| 1608/1608 [00:56<00:00, 28.54it/s]\n",
      "[INFO] MF-Morec-Finetune: epoch 24 training [time: 56.34s, train loss: 224.9737]\n",
      "[INFO] MF-Morec-Finetune: one_vs_all\n",
      "Evaluate: 100%|██████████| 122/122 [01:19<00:00,  1.54it/s]\n",
      "[INFO] MF-Morec-Finetune: epoch 24 evaluating [time: 79.44s, ndcg@10: 0.016027]\n",
      "[INFO] MF-Morec-Finetune: complete scores on valid set: \n",
      "hit@10:0.028539178327836305 min-hit@10:0.014145810663764961 min-ndcg@10:0.007837890336959261 min-rhit@10:0.48486035545883216 min-rndcg@10:0.27224024127709007 ndcg@10:0.016027466104496527 pop-kl@10:0.33821190248059274 rhit@10:3.4898359697716863 rndcg@10:1.999429702951893\n",
      "[INFO] MF-Morec-Finetune: Saving best model at epoch 24 to /home/v-huangxu/.unirec/output/Electronics/MF/morec_finetune_2023-10-28_11-35-22/MF-Morec-Finetune.pth\n",
      "[INFO] MF-Morec-Finetune: \n",
      ">> epoch 25\n",
      "Train: 100%|██████████| 1608/1608 [00:56<00:00, 28.51it/s]\n",
      "[INFO] MF-Morec-Finetune: epoch 25 training [time: 56.41s, train loss: 224.8876]\n",
      "[INFO] MF-Morec-Finetune: one_vs_all\n",
      "Evaluate: 100%|██████████| 122/122 [01:19<00:00,  1.54it/s]\n",
      "[INFO] MF-Morec-Finetune: epoch 25 evaluating [time: 79.32s, ndcg@10: 0.016182]\n",
      "[INFO] MF-Morec-Finetune: complete scores on valid set: \n",
      "hit@10:0.028819366614364853 min-hit@10:0.015233949945593036 min-ndcg@10:0.008037910179791231 min-rhit@10:0.4909249183895539 min-rndcg@10:0.26429113982158575 ndcg@10:0.01618233862748079 pop-kl@10:0.3485481717001484 rhit@10:3.486670082295302 rndcg@10:1.991265008903643\n",
      "[INFO] MF-Morec-Finetune: Saving best model at epoch 25 to /home/v-huangxu/.unirec/output/Electronics/MF/morec_finetune_2023-10-28_11-35-22/MF-Morec-Finetune.pth\n",
      "[INFO] MF-Morec-Finetune: \n",
      ">> epoch 26\n",
      "Train: 100%|██████████| 1608/1608 [00:56<00:00, 28.44it/s]\n",
      "[INFO] MF-Morec-Finetune: epoch 26 training [time: 56.55s, train loss: 224.8526]\n",
      "[INFO] MF-Morec-Finetune: one_vs_all\n",
      "Evaluate: 100%|██████████| 122/122 [01:19<00:00,  1.53it/s]\n",
      "[INFO] MF-Morec-Finetune: epoch 26 evaluating [time: 80.00s, ndcg@10: 0.015932]\n",
      "[INFO] MF-Morec-Finetune: complete scores on valid set: \n",
      "hit@10:0.028467129911300394 min-hit@10:0.01595937613347842 min-ndcg@10:0.00743725341573537 min-rhit@10:0.5224483133841131 min-rndcg@10:0.2518925643838487 ndcg@10:0.01593185266569287 pop-kl@10:0.3433993713609509 rhit@10:3.406581222581574 rndcg@10:1.9621277100237549\n",
      "[INFO] MF-Morec-Finetune: Saving best model at epoch 26 to /home/v-huangxu/.unirec/output/Electronics/MF/morec_finetune_2023-10-28_11-35-22/MF-Morec-Finetune.pth\n",
      "[INFO] MF-Morec-Finetune: \n",
      ">> epoch 27\n",
      "Train: 100%|██████████| 1608/1608 [00:57<00:00, 27.78it/s]\n",
      "[INFO] MF-Morec-Finetune: epoch 27 training [time: 57.89s, train loss: 224.7815]\n",
      "[INFO] MF-Morec-Finetune: one_vs_all\n",
      "Evaluate: 100%|██████████| 122/122 [01:21<00:00,  1.50it/s]\n",
      "[INFO] MF-Morec-Finetune: epoch 27 evaluating [time: 81.12s, ndcg@10: 0.016053]\n",
      "[INFO] MF-Morec-Finetune: complete scores on valid set: \n",
      "hit@10:0.028507156809375902 min-hit@10:0.015596663039535727 min-ndcg@10:0.007636920072597233 min-rhit@10:0.5016503445774393 min-rndcg@10:0.22730202905765945 ndcg@10:0.016052601433995943 pop-kl@10:0.3452702104736003 rhit@10:3.3889034231003237 rndcg@10:1.9533440088785592\n",
      "[INFO] MF-Morec-Finetune: Saving best model at epoch 27 to /home/v-huangxu/.unirec/output/Electronics/MF/morec_finetune_2023-10-28_11-35-22/MF-Morec-Finetune.pth\n",
      "[INFO] MF-Morec-Finetune: \n",
      ">> epoch 28\n",
      "Train: 100%|██████████| 1608/1608 [00:56<00:00, 28.42it/s]\n",
      "[INFO] MF-Morec-Finetune: epoch 28 training [time: 56.58s, train loss: 224.6369]\n",
      "[INFO] MF-Morec-Finetune: one_vs_all\n",
      "Evaluate: 100%|██████████| 122/122 [01:19<00:00,  1.54it/s]\n",
      "[INFO] MF-Morec-Finetune: epoch 28 evaluating [time: 79.35s, ndcg@10: 0.016046]\n",
      "[INFO] MF-Morec-Finetune: complete scores on valid set: \n",
      "hit@10:0.028755323577444043 min-hit@10:0.014871236851650345 min-ndcg@10:0.007638608678031306 min-rhit@10:0.4323286180631121 min-rndcg@10:0.21377970603392998 ndcg@10:0.01604555487038092 pop-kl@10:0.3499542168930102 rhit@10:3.5244501905280354 rndcg@10:1.9903732956085485\n",
      "[INFO] MF-Morec-Finetune: Saving best model at epoch 28 to /home/v-huangxu/.unirec/output/Electronics/MF/morec_finetune_2023-10-28_11-35-22/MF-Morec-Finetune.pth\n",
      "[INFO] MF-Morec-Finetune: \n",
      ">> epoch 29\n",
      "Train: 100%|██████████| 1608/1608 [00:56<00:00, 28.59it/s]\n",
      "[INFO] MF-Morec-Finetune: epoch 29 training [time: 56.24s, train loss: 224.4277]\n",
      "[INFO] MF-Morec-Finetune: one_vs_all\n",
      "Evaluate: 100%|██████████| 122/122 [01:19<00:00,  1.53it/s]\n",
      "[INFO] MF-Morec-Finetune: epoch 29 evaluating [time: 79.69s, ndcg@10: 0.016116]\n",
      "[INFO] MF-Morec-Finetune: complete scores on valid set: \n",
      "hit@10:0.02874731819782894 min-hit@10:0.01595937613347842 min-ndcg@10:0.007830146180290586 min-rhit@10:0.6946568473832654 min-rndcg@10:0.29277277421297776 ndcg@10:0.01611639535318425 pop-kl@10:0.3455934911012566 rhit@10:3.467173140350315 rndcg@10:1.9889973382004988\n",
      "[INFO] MF-Morec-Finetune: Saving best model at epoch 29 to /home/v-huangxu/.unirec/output/Electronics/MF/morec_finetune_2023-10-28_11-35-22/MF-Morec-Finetune.pth\n",
      "[INFO] MF-Morec-Finetune: \n",
      ">> epoch 30\n",
      "Train: 100%|██████████| 1608/1608 [00:56<00:00, 28.38it/s]\n",
      "[INFO] MF-Morec-Finetune: epoch 30 training [time: 56.67s, train loss: 224.5688]\n",
      "[INFO] MF-Morec-Finetune: Constructing dataset of task type: test\n",
      "[DEBUG] MF-Morec-Finetune: loading test at 28/10/2023 15:08:38\n",
      "[DEBUG] MF-Morec-Finetune: Finished loading test at 28/10/2023 15:08:38\n",
      "[INFO] MF-Morec-Finetune: Finished initializing <class 'unirec.data.dataset.basedataset.BaseDataset'>\n",
      "[INFO] MF-Morec-Finetune: one_vs_all\n",
      "[INFO] MF-Morec-Finetune: Loading model from /home/v-huangxu/.unirec/output/Electronics/MF/morec_finetune_2023-10-28_11-35-22/MF-Morec-Finetune.pth. The best epoch was 29\n",
      "Evaluate: 100%|██████████| 1250/1250 [02:25<00:00,  8.58it/s]\n",
      "[INFO] MF-Morec-Finetune: best valid : {'hit@10': 0.02874731819782894, 'ndcg@10': 0.01611639535318425, 'rhit@10': 3.467173140350315, 'rndcg@10': 1.9889973382004988, 'pop-kl@10': 0.3455934911012566, 'min-hit@10': 0.01595937613347842, 'min-ndcg@10': 0.007830146180290586, 'min-rhit@10': 0.6946568473832654, 'min-rndcg@10': 0.29277277421297776}\n",
      "[INFO] MF-Morec-Finetune: test result: {'hit@10': 0.016875340228633642, 'ndcg@10': 0.009093184074210331, 'rhit@10': 2.0894798104326107, 'rndcg@10': 1.0920012451897598, 'pop-kl@10': 0.34564678271559757, 'min-hit@10': 0.010013425822331617, 'min-ndcg@10': 0.0052509541140043185, 'min-rhit@10': 0.37056668158424705, 'min-rndcg@10': 0.17010550511085515}\n",
      "[INFO] MF-Morec-Finetune: Saving test result to /home/v-huangxu/.unirec/output/Electronics/MF/result_MF-Morec-Finetune.2023-10-28_140001.23.tsv ...\n",
      "[INFO] MF-Morec-Finetune: Mission complete. Time elapsed: 71.06 minutes.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logger close successfully.\n"
     ]
    }
   ],
   "source": [
    "# MoRec multi-objective post-training (fine-tuning) stage \n",
    "morec_config = deepcopy(GLOBAL_CONF)\n",
    "\n",
    "morec_config['enable_morec'] = 1\n",
    "morec_config['exp_name'] = 'Morec-Finetune'\n",
    "\n",
    "# pretrained model file is loaded by the `model_file` argument\n",
    "morec_config['model_file'] = os.path.join(pretrain_config['output_path'], pretrain_config['checkpoint_dir'], f\"{pretrain_config['model']}-{pretrain_config['exp_name']}.pth\")\n",
    "morec_config['checkpoint_dir'] = \"morec_finetune_\" + morec_config['checkpoint_dir']\n",
    "\n",
    "# MoRec parameters\n",
    "morec_config['morec_objectives']=['fairness', 'alignment', 'revenue']\n",
    "morec_config[\"morec_ngroup\"] = 40\n",
    "morec_config[\"morec_alpha\"] = 0.1\n",
    "morec_config[\"morec_lambda\"] = 0.2\n",
    "morec_config[\"morec_expect_loss\"] = 0.20\n",
    "morec_config[\"morec_beta_min\"] = 0.1\n",
    "morec_config[\"morec_beta_max\"] = 1.5\n",
    "morec_config[\"morec_K_p\"] = 0.05\n",
    "morec_config[\"morec_K_i\"] = 0.001\n",
    "morec_config[\"morec_objective_controller\"] = \"PID\"\n",
    "morec_config[\"morec_objective_weights\"] = \"[0.2,0.2,0.6]\"\n",
    "\n",
    "morec_config[\"epochs\"] = 30\n",
    "morec_config[\"early_stop\"] = -1\n",
    "\n",
    "morec_result = main.run(morec_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Performance Comparisons\n",
    "\n",
    "The MoRec framework could improve model's performance in revenue, alignment and fairness simultaneously, without sacrificing the recommendation accuracy.\n",
    "\n",
    "Note, the details of metrics used here are given in our [paper](https://arxiv.org/abs/2310.13260v1). The higher metrics represent the better performance, except the pop-kl.\n",
    "\n",
    "<table border=\"0\" cellpadding=\"0\" cellspacing=\"0\" width=\"544\" style=\"border-collapse:\n",
    " collapse;width:408pt\">\n",
    "  <thead>\n",
    "    <tr height=\"19\" style=\"height:13.9pt\">\n",
    "      <th>Objective</th>\n",
    "      <th colspan=\"2\" style=\"text-align:center\">Accuracy</th>\n",
    "      <th colspan=\"2\" style=\"text-align:center\">Revenue</th>\n",
    "      <th style=\"text-align:center\">Alignment</th>\n",
    "      <th colspan=\"2\" style=\"text-align:center\">Fairness</th>\n",
    "    </tr>\n",
    "  </thead>\n",
    " <colgroup><col width=\"68\" span=\"8\" style=\"width:51pt\">\n",
    " </colgroup>\n",
    "  <tbody>\n",
    "    <tr height=\"19\" style=\"height:13.9pt\">\n",
    "      <td height=\"19\" class=\"xl65\" style=\"height:13.9pt\">Metric</td>\n",
    "      <td class=\"xl65\">hit@10</td>\n",
    "      <td class=\"xl67\">ndcg@10</td>\n",
    "      <td class=\"xl65\" style=\"border-left:none\">rhit@10</td>\n",
    "      <td class=\"xl67\">rndcg@10</td>\n",
    "      <td class=\"xl72\" style=\"border-left:none\">pop-kl@10</td>\n",
    "      <td class=\"xl66\">min-hit@10</td>\n",
    "      <td class=\"xl67\">min-ndcg@10</td>\n",
    "    </tr>\n",
    "    <tr height=\"19\" style=\"height:13.9pt\">\n",
    "      <td height=\"19\" class=\"xl64\" style=\"height:13.9pt\">Pretrain</td>\n",
    "      <td class=\"xl73\" align=\"right\">0.0162</td>\n",
    "      <td class=\"xl74\" align=\"right\">0.0088</td>\n",
    "      <td class=\"xl73\" align=\"right\" style=\"border-left:none\">1.3542</td>\n",
    "      <td class=\"xl74\" align=\"right\">0.6982</td>\n",
    "      <td class=\"xl75\" align=\"right\" style=\"border-left:none\">1.4301</td>\n",
    "      <td class=\"xl76\" align=\"right\">0.0091</td>\n",
    "      <td class=\"xl74\" align=\"right\">0.0052</td>\n",
    "    </tr>\n",
    "    <tr height=\"19\" style=\"height:13.9pt\">\n",
    "      <td height=\"19\" class=\"xl65\" style=\"height:13.9pt\">MoRec</td>\n",
    "      <td class=\"xl77\" align=\"right\">0.0169</td>\n",
    "      <td class=\"xl78\" align=\"right\">0.0091</td>\n",
    "      <td class=\"xl77\" align=\"right\" style=\"border-left:none\">2.0895</td>\n",
    "      <td class=\"xl78\" align=\"right\">1.0920</td>\n",
    "      <td class=\"xl79\" align=\"right\" style=\"border-left:none\">0.3456</td>\n",
    "      <td class=\"xl80\" align=\"right\">0.0100</td>\n",
    "      <td class=\"xl78\" align=\"right\">0.0053</td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretrain:  {'hit@10': 0.016234909859425533, 'ndcg@10': 0.008779203758959964, 'rhit@10': 1.3542233180697432, 'rndcg@10': 0.6982034604612465, 'pop-kl@10': 1.4300518294507298, 'min-hit@10': 0.009118371000223764, 'min-ndcg@10': 0.00516860580638276, 'min-rhit@10': 0.21157921235175656, 'min-rndcg@10': 0.12419234342959445}\n",
      "Finetune:  {'hit@10': 0.016875340228633642, 'ndcg@10': 0.009093184074210331, 'rhit@10': 2.0894798104326107, 'rndcg@10': 1.0920012451897598, 'pop-kl@10': 0.34564678271559757, 'min-hit@10': 0.010013425822331617, 'min-ndcg@10': 0.0052509541140043185, 'min-rhit@10': 0.37056668158424705, 'min-rndcg@10': 0.17010550511085515}\n"
     ]
    }
   ],
   "source": [
    "print(\"Pretrain: \", pretrain_result)\n",
    "print(\"Finetune: \", morec_result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unirec",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
